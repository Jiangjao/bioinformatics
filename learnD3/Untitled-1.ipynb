{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Phylum   T1   T2   T3   T4   T5   T6   T7   T8\n",
      "0         Bacteroidetes   15   40   80  130   66   44   10   15\n",
      "1            Firmicutes  120  110   77   90   79  130  120  110\n",
      "2        Proteobacteria  130  230  330  190  180  190  170  130\n",
      "3           Thermotogae  280  290  350  200  190  180  128   99\n",
      "4  Coprothermobacterota  200  270  420  330  420  270  210  130\n",
      "                    0           1               2            3  \\\n",
      "Phylum  Bacteroidetes  Firmicutes  Proteobacteria  Thermotogae   \n",
      "T1                 15         120             130          280   \n",
      "T2                 40         110             230          290   \n",
      "T3                 80          77             330          350   \n",
      "T4                130          90             190          200   \n",
      "T5                 66          79             180          190   \n",
      "T6                 44         130             190          180   \n",
      "T7                 10         120             170          128   \n",
      "T8                 15         110             130           99   \n",
      "\n",
      "                           4  \n",
      "Phylum  Coprothermobacterota  \n",
      "T1                       200  \n",
      "T2                       270  \n",
      "T3                       420  \n",
      "T4                       330  \n",
      "T5                       420  \n",
      "T6                       270  \n",
      "T7                       210  \n",
      "T8                       130  \n",
      "[{0: 15, 1: 120, 2: 130, 3: 280, 4: 200}, {0: 40, 1: 110, 2: 230, 3: 290, 4: 270}, {0: 80, 1: 77, 2: 330, 3: 350, 4: 420}, {0: 130, 1: 90, 2: 190, 3: 200, 4: 330}, {0: 66, 1: 79, 2: 180, 3: 190, 4: 420}, {0: 44, 1: 130, 2: 190, 3: 180, 4: 270}, {0: 10, 1: 120, 2: 170, 3: 128, 4: 210}, {0: 15, 1: 110, 2: 130, 3: 99, 4: 130}]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from jinja2 import Environment, FileSystemLoader\n",
    "# https://zhuanlan.zhihu.com/p/384252746\n",
    "\n",
    "\n",
    "data = \"\"\"[{ year: 2010, apples: 10, oranges: 15, bananas: 20 },\n",
    "            { year: 2011, apples: 12, oranges: 18, bananas: 22 },\n",
    "            { year: 2012, apples: 8, oranges: 16, bananas: 24 },\n",
    "            { year: 2013, apples: 15, oranges: 12, bananas: 18 },\n",
    "            { year: 2014, apples: 20, oranges: 10, bananas: 16 }\n",
    "        ]\"\"\"\n",
    "\n",
    "with open(\"./stack_line_wide_data.eg.csv\", 'r+', encoding='utf-8') as f:\n",
    "\n",
    "    df = pd.read_csv(\"./stack_line_wide_data.eg.csv\", encoding=\"utf8\", header=0)\n",
    "    # 去除每一行前面的索引\n",
    "    # df = df.reset_index(drop=True)\n",
    "    print(df)\n",
    "    # print(df.T)\n",
    "    df = df.T\n",
    "    print(df)\n",
    "    # df = df.reset_index(drop=True)\n",
    "    # 使用 df[1:] 去除第一行\n",
    "    df = df[1:]\n",
    "    # df = df.T\n",
    "    # 转置 DataFrame 并重新排列\n",
    "    # df = df.pivot(index= range(len(data[0])-1) , columns='Phylum')\n",
    "    data = df.to_dict(orient='records')\n",
    "    print(data)\n",
    "\n",
    "\n",
    "# if data != None:\n",
    "#     data = f\"{data}\"\n",
    "\n",
    "# env = Environment(loader=FileSystemLoader('./'))\n",
    "\n",
    "# template = env.get_template('test_stack_stack.html')\n",
    "\n",
    "# with open(\"out.html\", 'w+', encoding='utf-8') as f:\n",
    "#     out = template.render(data=data)\n",
    "#     f.write(out)\n",
    "#     f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Sample': 'Pseudomonadaceae', 'C1': 666, 'C2': 764, 'C3': 39, 'T1': 636, 'T2': 579, 'T3': 10, 'M1': 180, 'M2': 726, 'M3': 441, 'total': 1482, 'C_mean': 489.6666666666667, 'T_mean': 408.3333333333333, 'M_mean': 449.0}, {'Sample': 'Enterobacteriaceae', 'C1': 723, 'C2': 828, 'C3': 170, 'T1': 79, 'T2': 130, 'T3': 120, 'M1': 110, 'M2': 108, 'M3': 106, 'total': 912, 'C_mean': 573.6666666666666, 'T_mean': 109.66666666666667, 'M_mean': 108.0}, {'Sample': 'Comamonadaceae', 'C1': 780, 'C2': 892, 'C3': 330, 'T1': 180, 'T2': 190, 'T3': 170, 'M1': 187, 'M2': 183, 'M3': 90, 'total': 1147, 'C_mean': 667.3333333333334, 'T_mean': 180.0, 'M_mean': 153.33333333333334}, {'Sample': 'Flavobacteriaceae', 'C1': 280, 'C2': 290, 'C3': 350, 'T1': 300, 'T2': 180, 'T3': 160, 'M1': 173, 'M2': 150, 'M3': 190, 'total': 753, 'C_mean': 306.6666666666667, 'T_mean': 213.33333333333334, 'M_mean': 171.0}, {'Sample': 'Bacteroidaceae', 'C1': 220, 'C2': 270, 'C3': 420, 'T1': 420, 'T2': 270, 'T3': 210, 'M1': 301, 'M2': 300, 'M3': 79, 'total': 941, 'C_mean': 303.3333333333333, 'T_mean': 300.0, 'M_mean': 226.66666666666666}, {'Sample': 'Bradyrhizobiaceae', 'C1': 324, 'C2': 380, 'C3': 544, 'T1': 509, 'T2': 327, 'T3': 266, 'M1': 351, 'M2': 340, 'M3': 328, 'total': 1184, 'C_mean': 416.0, 'T_mean': 367.3333333333333, 'M_mean': 339.6666666666667}, {'Sample': 'Rhodobacteraceae', 'C1': 381, 'C2': 444, 'C3': 639, 'T1': 612, 'T2': 384, 'T3': 310, 'M1': 416, 'M2': 403, 'M3': 390, 'total': 1409, 'C_mean': 488.0, 'T_mean': 435.3333333333333, 'M_mean': 403.0}, {'Sample': 'Zoogloeaceae', 'C1': 438, 'C2': 508, 'C3': 733, 'T1': 716, 'T2': 441, 'T3': 354, 'M1': 481, 'M2': 466, 'M3': 452, 'total': 1635, 'C_mean': 559.6666666666666, 'T_mean': 503.6666666666667, 'M_mean': 466.3333333333333}, {'Sample': 'Cyclobacteriaceae', 'C1': 495, 'C2': 572, 'C3': 324, 'T1': 819, 'T2': 498, 'T3': 398, 'M1': 546, 'M2': 39, 'M3': 514, 'total': 1860, 'C_mean': 463.6666666666667, 'T_mean': 571.6666666666666, 'M_mean': 366.3333333333333}, {'Sample': 'Lachnospiraceae', 'C1': 552, 'C2': 636, 'C3': 381, 'T1': 923, 'T2': 800, 'T3': 700, 'M1': 780, 'M2': 870, 'M3': 828, 'total': 2255, 'C_mean': 523.0, 'T_mean': 807.6666666666666, 'M_mean': 826.0}, {'Sample': 'Rhizobiaceae', 'C1': 970, 'C2': 890, 'C3': 980, 'T1': 1026, 'T2': 1000, 'T3': 790, 'M1': 870, 'M2': 870, 'M3': 892, 'total': 2866, 'C_mean': 946.6666666666666, 'T_mean': 938.6666666666666, 'M_mean': 877.3333333333334}, {'Sample': 'Acidaminococcaceae', 'C1': 253, 'C2': 240, 'C3': 270, 'T1': 170, 'T2': 442, 'T3': 530, 'M1': 498, 'M2': 350, 'M3': 290, 'total': 921, 'C_mean': 254.33333333333334, 'T_mean': 380.6666666666667, 'M_mean': 379.3333333333333}, {'Sample': 'Aeromonadaceae', 'C1': 3200, 'C2': 3300, 'C3': 3560, 'T1': 3510, 'T2': 3910, 'T3': 3780, 'M1': 3587, 'M2': 3568, 'M3': 3145, 'total': 10297, 'C_mean': 3353.3333333333335, 'T_mean': 3733.3333333333335, 'M_mean': 3433.3333333333335}, {'Sample': 'Campylobacteraceae', 'C1': 2400, 'C2': 2400, 'C3': 2300, 'T1': 2400, 'T2': 2300, 'T3': 2800, 'M1': 2600, 'M2': 2500, 'M3': 2670, 'total': 7400, 'C_mean': 2366.6666666666665, 'T_mean': 2500.0, 'M_mean': 2590.0}, {'Sample': 'Bacillaceae', 'C1': 837, 'C2': 956, 'C3': 780, 'T1': 266, 'T2': 840, 'T3': 662, 'M1': 669, 'M2': 421, 'M3': 885, 'total': 1772, 'C_mean': 857.6666666666666, 'T_mean': 589.3333333333334, 'M_mean': 658.3333333333334}, {'Sample': 'Ruminococcaceae', 'C1': 894, 'C2': 110, 'C3': 280, 'T1': 310, 'T2': 897, 'T3': 706, 'M1': 726, 'M2': 486, 'M3': 120, 'total': 1930, 'C_mean': 428.0, 'T_mean': 637.6666666666666, 'M_mean': 444.0}, {'Sample': 'Burkholderiaceae', 'C1': 220, 'C2': 110, 'C3': 220, 'T1': 354, 'T2': 954, 'T3': 750, 'M1': 669, 'M2': 530, 'M3': 170, 'total': 1243, 'C_mean': 183.33333333333334, 'T_mean': 686.0, 'M_mean': 456.3333333333333}, {'Sample': 'Moraxellaceae', 'C1': 381, 'C2': 892, 'C3': 280, 'T1': 398, 'T2': 300, 'T3': 794, 'M1': 726, 'M2': 574, 'M3': 160, 'total': 1505, 'C_mean': 517.6666666666666, 'T_mean': 497.3333333333333, 'M_mean': 486.6666666666667}, {'Sample': 'Clostridiaceae', 'C1': 438, 'C2': 290, 'C3': 330, 'T1': 183, 'T2': 420, 'T3': 50, 'M1': 380, 'M2': 190, 'M3': 120, 'total': 1001, 'C_mean': 352.6666666666667, 'T_mean': 217.66666666666666, 'M_mean': 230.0}, {'Sample': 'Microbacteriaceae', 'C1': 495, 'C2': 270, 'C3': 421, 'T1': 150, 'T2': 39, 'T3': 101, 'M1': 444, 'M2': 180, 'M3': 170, 'total': 1089, 'C_mean': 395.3333333333333, 'T_mean': 96.66666666666667, 'M_mean': 264.6666666666667}, {'Sample': 'Sphingomonadaceae', 'C1': 552, 'C2': 380, 'C3': 500, 'T1': 300, 'T2': 170, 'T3': 110, 'M1': 190, 'M2': 190, 'M3': 120, 'total': 1042, 'C_mean': 477.3333333333333, 'T_mean': 193.33333333333334, 'M_mean': 166.66666666666666}, {'Sample': 'Prevotellaceae', 'C1': 280, 'C2': 100, 'C3': 330, 'T1': 170, 'T2': 101, 'T3': 100, 'M1': 180, 'M2': 180, 'M3': 170, 'total': 630, 'C_mean': 236.66666666666666, 'T_mean': 123.66666666666667, 'M_mean': 176.66666666666666}, {'Sample': 'Gordoniaceae', 'C1': 220, 'C2': 170, 'C3': 350, 'T1': 220, 'T2': 102, 'T3': 723, 'M1': 270, 'M2': 270, 'M3': 160, 'total': 710, 'C_mean': 246.66666666666666, 'T_mean': 348.3333333333333, 'M_mean': 233.33333333333334}, {'Sample': 'Erysipelotrichaceae', 'C1': 324, 'C2': 330, 'C3': 420, 'T1': 220, 'T2': 103, 'T3': 723, 'M1': 327, 'M2': 327, 'M3': 210, 'total': 871, 'C_mean': 358.0, 'T_mean': 348.6666666666667, 'M_mean': 288.0}, {'Sample': 'Tannerellaceae', 'C1': 381, 'C2': 350, 'C3': 330, 'T1': 280, 'T2': 290, 'T3': 110, 'M1': 384, 'M2': 384, 'M3': 266, 'total': 1045, 'C_mean': 353.6666666666667, 'T_mean': 226.66666666666666, 'M_mean': 344.6666666666667}, {'Sample': 'Xanthomonadaceae', 'C1': 438, 'C2': 420, 'C3': 350, 'T1': 330, 'T2': 270, 'T3': 280, 'M1': 441, 'M2': 441, 'M3': 310, 'total': 1209, 'C_mean': 402.6666666666667, 'T_mean': 293.3333333333333, 'M_mean': 397.3333333333333}, {'Sample': 'Caulobacteraceae', 'C1': 121, 'C2': 120, 'C3': 140, 'T1': 421, 'T2': 380, 'T3': 220, 'M1': 180, 'M2': 200, 'M3': 354, 'total': 722, 'C_mean': 127.0, 'T_mean': 340.3333333333333, 'M_mean': 244.66666666666666}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'Sample': 'Pseudomonadaceae',\n",
       "  'C1': 666,\n",
       "  'C2': 764,\n",
       "  'C3': 39,\n",
       "  'T1': 636,\n",
       "  'T2': 579,\n",
       "  'T3': 10,\n",
       "  'M1': 180,\n",
       "  'M2': 726,\n",
       "  'M3': 441,\n",
       "  'total': 1482,\n",
       "  'C_mean': 489.6666666666667,\n",
       "  'T_mean': 408.3333333333333,\n",
       "  'M_mean': 449.0},\n",
       " {'Sample': 'Enterobacteriaceae',\n",
       "  'C1': 723,\n",
       "  'C2': 828,\n",
       "  'C3': 170,\n",
       "  'T1': 79,\n",
       "  'T2': 130,\n",
       "  'T3': 120,\n",
       "  'M1': 110,\n",
       "  'M2': 108,\n",
       "  'M3': 106,\n",
       "  'total': 912,\n",
       "  'C_mean': 573.6666666666666,\n",
       "  'T_mean': 109.66666666666667,\n",
       "  'M_mean': 108.0},\n",
       " {'Sample': 'Comamonadaceae',\n",
       "  'C1': 780,\n",
       "  'C2': 892,\n",
       "  'C3': 330,\n",
       "  'T1': 180,\n",
       "  'T2': 190,\n",
       "  'T3': 170,\n",
       "  'M1': 187,\n",
       "  'M2': 183,\n",
       "  'M3': 90,\n",
       "  'total': 1147,\n",
       "  'C_mean': 667.3333333333334,\n",
       "  'T_mean': 180.0,\n",
       "  'M_mean': 153.33333333333334},\n",
       " {'Sample': 'Flavobacteriaceae',\n",
       "  'C1': 280,\n",
       "  'C2': 290,\n",
       "  'C3': 350,\n",
       "  'T1': 300,\n",
       "  'T2': 180,\n",
       "  'T3': 160,\n",
       "  'M1': 173,\n",
       "  'M2': 150,\n",
       "  'M3': 190,\n",
       "  'total': 753,\n",
       "  'C_mean': 306.6666666666667,\n",
       "  'T_mean': 213.33333333333334,\n",
       "  'M_mean': 171.0},\n",
       " {'Sample': 'Bacteroidaceae',\n",
       "  'C1': 220,\n",
       "  'C2': 270,\n",
       "  'C3': 420,\n",
       "  'T1': 420,\n",
       "  'T2': 270,\n",
       "  'T3': 210,\n",
       "  'M1': 301,\n",
       "  'M2': 300,\n",
       "  'M3': 79,\n",
       "  'total': 941,\n",
       "  'C_mean': 303.3333333333333,\n",
       "  'T_mean': 300.0,\n",
       "  'M_mean': 226.66666666666666},\n",
       " {'Sample': 'Bradyrhizobiaceae',\n",
       "  'C1': 324,\n",
       "  'C2': 380,\n",
       "  'C3': 544,\n",
       "  'T1': 509,\n",
       "  'T2': 327,\n",
       "  'T3': 266,\n",
       "  'M1': 351,\n",
       "  'M2': 340,\n",
       "  'M3': 328,\n",
       "  'total': 1184,\n",
       "  'C_mean': 416.0,\n",
       "  'T_mean': 367.3333333333333,\n",
       "  'M_mean': 339.6666666666667},\n",
       " {'Sample': 'Rhodobacteraceae',\n",
       "  'C1': 381,\n",
       "  'C2': 444,\n",
       "  'C3': 639,\n",
       "  'T1': 612,\n",
       "  'T2': 384,\n",
       "  'T3': 310,\n",
       "  'M1': 416,\n",
       "  'M2': 403,\n",
       "  'M3': 390,\n",
       "  'total': 1409,\n",
       "  'C_mean': 488.0,\n",
       "  'T_mean': 435.3333333333333,\n",
       "  'M_mean': 403.0},\n",
       " {'Sample': 'Zoogloeaceae',\n",
       "  'C1': 438,\n",
       "  'C2': 508,\n",
       "  'C3': 733,\n",
       "  'T1': 716,\n",
       "  'T2': 441,\n",
       "  'T3': 354,\n",
       "  'M1': 481,\n",
       "  'M2': 466,\n",
       "  'M3': 452,\n",
       "  'total': 1635,\n",
       "  'C_mean': 559.6666666666666,\n",
       "  'T_mean': 503.6666666666667,\n",
       "  'M_mean': 466.3333333333333},\n",
       " {'Sample': 'Cyclobacteriaceae',\n",
       "  'C1': 495,\n",
       "  'C2': 572,\n",
       "  'C3': 324,\n",
       "  'T1': 819,\n",
       "  'T2': 498,\n",
       "  'T3': 398,\n",
       "  'M1': 546,\n",
       "  'M2': 39,\n",
       "  'M3': 514,\n",
       "  'total': 1860,\n",
       "  'C_mean': 463.6666666666667,\n",
       "  'T_mean': 571.6666666666666,\n",
       "  'M_mean': 366.3333333333333},\n",
       " {'Sample': 'Lachnospiraceae',\n",
       "  'C1': 552,\n",
       "  'C2': 636,\n",
       "  'C3': 381,\n",
       "  'T1': 923,\n",
       "  'T2': 800,\n",
       "  'T3': 700,\n",
       "  'M1': 780,\n",
       "  'M2': 870,\n",
       "  'M3': 828,\n",
       "  'total': 2255,\n",
       "  'C_mean': 523.0,\n",
       "  'T_mean': 807.6666666666666,\n",
       "  'M_mean': 826.0},\n",
       " {'Sample': 'Rhizobiaceae',\n",
       "  'C1': 970,\n",
       "  'C2': 890,\n",
       "  'C3': 980,\n",
       "  'T1': 1026,\n",
       "  'T2': 1000,\n",
       "  'T3': 790,\n",
       "  'M1': 870,\n",
       "  'M2': 870,\n",
       "  'M3': 892,\n",
       "  'total': 2866,\n",
       "  'C_mean': 946.6666666666666,\n",
       "  'T_mean': 938.6666666666666,\n",
       "  'M_mean': 877.3333333333334},\n",
       " {'Sample': 'Acidaminococcaceae',\n",
       "  'C1': 253,\n",
       "  'C2': 240,\n",
       "  'C3': 270,\n",
       "  'T1': 170,\n",
       "  'T2': 442,\n",
       "  'T3': 530,\n",
       "  'M1': 498,\n",
       "  'M2': 350,\n",
       "  'M3': 290,\n",
       "  'total': 921,\n",
       "  'C_mean': 254.33333333333334,\n",
       "  'T_mean': 380.6666666666667,\n",
       "  'M_mean': 379.3333333333333},\n",
       " {'Sample': 'Aeromonadaceae',\n",
       "  'C1': 3200,\n",
       "  'C2': 3300,\n",
       "  'C3': 3560,\n",
       "  'T1': 3510,\n",
       "  'T2': 3910,\n",
       "  'T3': 3780,\n",
       "  'M1': 3587,\n",
       "  'M2': 3568,\n",
       "  'M3': 3145,\n",
       "  'total': 10297,\n",
       "  'C_mean': 3353.3333333333335,\n",
       "  'T_mean': 3733.3333333333335,\n",
       "  'M_mean': 3433.3333333333335},\n",
       " {'Sample': 'Campylobacteraceae',\n",
       "  'C1': 2400,\n",
       "  'C2': 2400,\n",
       "  'C3': 2300,\n",
       "  'T1': 2400,\n",
       "  'T2': 2300,\n",
       "  'T3': 2800,\n",
       "  'M1': 2600,\n",
       "  'M2': 2500,\n",
       "  'M3': 2670,\n",
       "  'total': 7400,\n",
       "  'C_mean': 2366.6666666666665,\n",
       "  'T_mean': 2500.0,\n",
       "  'M_mean': 2590.0},\n",
       " {'Sample': 'Bacillaceae',\n",
       "  'C1': 837,\n",
       "  'C2': 956,\n",
       "  'C3': 780,\n",
       "  'T1': 266,\n",
       "  'T2': 840,\n",
       "  'T3': 662,\n",
       "  'M1': 669,\n",
       "  'M2': 421,\n",
       "  'M3': 885,\n",
       "  'total': 1772,\n",
       "  'C_mean': 857.6666666666666,\n",
       "  'T_mean': 589.3333333333334,\n",
       "  'M_mean': 658.3333333333334},\n",
       " {'Sample': 'Ruminococcaceae',\n",
       "  'C1': 894,\n",
       "  'C2': 110,\n",
       "  'C3': 280,\n",
       "  'T1': 310,\n",
       "  'T2': 897,\n",
       "  'T3': 706,\n",
       "  'M1': 726,\n",
       "  'M2': 486,\n",
       "  'M3': 120,\n",
       "  'total': 1930,\n",
       "  'C_mean': 428.0,\n",
       "  'T_mean': 637.6666666666666,\n",
       "  'M_mean': 444.0},\n",
       " {'Sample': 'Burkholderiaceae',\n",
       "  'C1': 220,\n",
       "  'C2': 110,\n",
       "  'C3': 220,\n",
       "  'T1': 354,\n",
       "  'T2': 954,\n",
       "  'T3': 750,\n",
       "  'M1': 669,\n",
       "  'M2': 530,\n",
       "  'M3': 170,\n",
       "  'total': 1243,\n",
       "  'C_mean': 183.33333333333334,\n",
       "  'T_mean': 686.0,\n",
       "  'M_mean': 456.3333333333333},\n",
       " {'Sample': 'Moraxellaceae',\n",
       "  'C1': 381,\n",
       "  'C2': 892,\n",
       "  'C3': 280,\n",
       "  'T1': 398,\n",
       "  'T2': 300,\n",
       "  'T3': 794,\n",
       "  'M1': 726,\n",
       "  'M2': 574,\n",
       "  'M3': 160,\n",
       "  'total': 1505,\n",
       "  'C_mean': 517.6666666666666,\n",
       "  'T_mean': 497.3333333333333,\n",
       "  'M_mean': 486.6666666666667},\n",
       " {'Sample': 'Clostridiaceae',\n",
       "  'C1': 438,\n",
       "  'C2': 290,\n",
       "  'C3': 330,\n",
       "  'T1': 183,\n",
       "  'T2': 420,\n",
       "  'T3': 50,\n",
       "  'M1': 380,\n",
       "  'M2': 190,\n",
       "  'M3': 120,\n",
       "  'total': 1001,\n",
       "  'C_mean': 352.6666666666667,\n",
       "  'T_mean': 217.66666666666666,\n",
       "  'M_mean': 230.0},\n",
       " {'Sample': 'Microbacteriaceae',\n",
       "  'C1': 495,\n",
       "  'C2': 270,\n",
       "  'C3': 421,\n",
       "  'T1': 150,\n",
       "  'T2': 39,\n",
       "  'T3': 101,\n",
       "  'M1': 444,\n",
       "  'M2': 180,\n",
       "  'M3': 170,\n",
       "  'total': 1089,\n",
       "  'C_mean': 395.3333333333333,\n",
       "  'T_mean': 96.66666666666667,\n",
       "  'M_mean': 264.6666666666667},\n",
       " {'Sample': 'Sphingomonadaceae',\n",
       "  'C1': 552,\n",
       "  'C2': 380,\n",
       "  'C3': 500,\n",
       "  'T1': 300,\n",
       "  'T2': 170,\n",
       "  'T3': 110,\n",
       "  'M1': 190,\n",
       "  'M2': 190,\n",
       "  'M3': 120,\n",
       "  'total': 1042,\n",
       "  'C_mean': 477.3333333333333,\n",
       "  'T_mean': 193.33333333333334,\n",
       "  'M_mean': 166.66666666666666},\n",
       " {'Sample': 'Prevotellaceae',\n",
       "  'C1': 280,\n",
       "  'C2': 100,\n",
       "  'C3': 330,\n",
       "  'T1': 170,\n",
       "  'T2': 101,\n",
       "  'T3': 100,\n",
       "  'M1': 180,\n",
       "  'M2': 180,\n",
       "  'M3': 170,\n",
       "  'total': 630,\n",
       "  'C_mean': 236.66666666666666,\n",
       "  'T_mean': 123.66666666666667,\n",
       "  'M_mean': 176.66666666666666},\n",
       " {'Sample': 'Gordoniaceae',\n",
       "  'C1': 220,\n",
       "  'C2': 170,\n",
       "  'C3': 350,\n",
       "  'T1': 220,\n",
       "  'T2': 102,\n",
       "  'T3': 723,\n",
       "  'M1': 270,\n",
       "  'M2': 270,\n",
       "  'M3': 160,\n",
       "  'total': 710,\n",
       "  'C_mean': 246.66666666666666,\n",
       "  'T_mean': 348.3333333333333,\n",
       "  'M_mean': 233.33333333333334},\n",
       " {'Sample': 'Erysipelotrichaceae',\n",
       "  'C1': 324,\n",
       "  'C2': 330,\n",
       "  'C3': 420,\n",
       "  'T1': 220,\n",
       "  'T2': 103,\n",
       "  'T3': 723,\n",
       "  'M1': 327,\n",
       "  'M2': 327,\n",
       "  'M3': 210,\n",
       "  'total': 871,\n",
       "  'C_mean': 358.0,\n",
       "  'T_mean': 348.6666666666667,\n",
       "  'M_mean': 288.0},\n",
       " {'Sample': 'Tannerellaceae',\n",
       "  'C1': 381,\n",
       "  'C2': 350,\n",
       "  'C3': 330,\n",
       "  'T1': 280,\n",
       "  'T2': 290,\n",
       "  'T3': 110,\n",
       "  'M1': 384,\n",
       "  'M2': 384,\n",
       "  'M3': 266,\n",
       "  'total': 1045,\n",
       "  'C_mean': 353.6666666666667,\n",
       "  'T_mean': 226.66666666666666,\n",
       "  'M_mean': 344.6666666666667},\n",
       " {'Sample': 'Xanthomonadaceae',\n",
       "  'C1': 438,\n",
       "  'C2': 420,\n",
       "  'C3': 350,\n",
       "  'T1': 330,\n",
       "  'T2': 270,\n",
       "  'T3': 280,\n",
       "  'M1': 441,\n",
       "  'M2': 441,\n",
       "  'M3': 310,\n",
       "  'total': 1209,\n",
       "  'C_mean': 402.6666666666667,\n",
       "  'T_mean': 293.3333333333333,\n",
       "  'M_mean': 397.3333333333333},\n",
       " {'Sample': 'Caulobacteraceae',\n",
       "  'C1': 121,\n",
       "  'C2': 120,\n",
       "  'C3': 140,\n",
       "  'T1': 421,\n",
       "  'T2': 380,\n",
       "  'T3': 220,\n",
       "  'M1': 180,\n",
       "  'M2': 200,\n",
       "  'M3': 354,\n",
       "  'total': 722,\n",
       "  'C_mean': 127.0,\n",
       "  'T_mean': 340.3333333333333,\n",
       "  'M_mean': 244.66666666666666}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn import datasets\n",
    "import pandas as pd\n",
    "\n",
    "# https://blog.csdn.net/Mr_blueD/article/details/81153044\n",
    "# iris_datas = datasets.load_iris()\n",
    "\n",
    "# iris = pd.DataFrame(iris_datas.data, columns=['SpealLength', 'Spealwidth', 'PetalWidth', 'PetalLength'])\n",
    "\n",
    "# iris = pd.read_csv(\"./iris.csv\")\n",
    "# iris\n",
    "# # 将数据加载成DataFrame\n",
    "# df = pd.DataFrame(iris_datas.data, columns=iris_datas.feature_names)\n",
    "\n",
    "# df.shape\n",
    "# df.head()\n",
    "\n",
    "# iris = pd.DataFrame(iris, columns=['species' ,'sepal_length', 'sepal_width', 'petal_length', 'petal_width'])\n",
    "\n",
    "# 随机采样10条数据\n",
    "# iris = iris.sample(n=100)\n",
    "\n",
    "# 使用applymap直接对dataframe每个元素应用str函数:\n",
    "# iris = iris.applymap(str)\n",
    "\n",
    "# # 转换成字典列表\n",
    "# data = iris.head().to_dict('records')\n",
    "\n",
    "# data\n",
    "# iris_datas.species\n",
    "# df = pd.read_table('./acchistogram_input_modified.txt', sep='/t')\n",
    "\n",
    "# df.head().to_dict('records')\n",
    "# pd.read_csv(\"./acchistogram_input.txt\", delimiter=\"\")\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# 读取数据文件\n",
    "df = pd.read_table('./acchistogram_input_modified.txt', sep='/t')\n",
    "\n",
    "# 读取分组情况表  \n",
    "group = pd.read_table('./accternary_map.tsv', sep=\"/t\")\n",
    "\n",
    "# 获取主分组名\n",
    "# groups = list(group.iloc[0,:])\n",
    "groups = list(group.columns)\n",
    "\n",
    "\n",
    "# 计算每个Sample的总和\n",
    "df['total'] = df[group.iloc[0,:]].sum(axis=1)\n",
    "\n",
    "# 求各主分组在每个Sample中的平均值\n",
    "for g in groups:\n",
    "    # print(g, \">>>\")\n",
    "    # 直接读取分组表对应主分组的子列名\n",
    "    sub_cols = group[g].tolist()\n",
    "    # sub_cols = [col for col in df.columns if col.startswith(g)]\n",
    "    # print(sub_cols)\n",
    "    df[g+'_mean'] = df[sub_cols].mean(axis=1)\n",
    "\n",
    "# # 求各主分组在每个Sample中的相对丰度\n",
    "# for g in groups:\n",
    "#     df[g+'_relative'] = df[g].div(df['total'], axis=0)\n",
    "\n",
    "print(df.to_dict(\"records\"))\n",
    "\n",
    "# 计算每行和,添加到新列'total'\n",
    "# df['total'] = df.sum(axis=1) \n",
    "\n",
    "# 获取总和\n",
    "total = df['total'].sum()\n",
    "df.to_dict(\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    " \n",
    "# Data set\n",
    "url = 'https://raw.githubusercontent.com/holtzy/The-Python-Graph-Gallery/master/static/data/mtcars.csv'\n",
    "df = pd.read_csv(url)\n",
    "df = df.set_index('model')\n",
    " \n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import numpy as np\n",
    "from Bio import Phylo\n",
    "# 创建样本数据\n",
    "X = np.array([[1, 2], [1, 4], [1, 0],\n",
    "              [4, 2], [4, 4], [4, 0]])\n",
    "\n",
    "# 创建层次聚类模型\n",
    "clustering = AgglomerativeClustering(distance_threshold=0, n_clusters=None)\n",
    "\n",
    "# 进行层次聚类\n",
    "clustering.fit(X)\n",
    "\n",
    "# 获取聚类结果\n",
    "labels = clustering.labels_\n",
    "\n",
    "# 打印每个样本的聚类标签\n",
    "print(labels)\n",
    "\n",
    "print(clustering.distances_)\n",
    "\n",
    "# 打印每个数据点及其聚类标签\n",
    "for i in range(len(X)):\n",
    "    print(\"Data point:\", X[i], \"Cluster label:\", labels[i])\n",
    "\n",
    "\n",
    "\n",
    "# 创建叶节点\n",
    "leaves = []\n",
    "for i in range(len(X)):\n",
    "    leaves.append(Phylo.BaseTree.Clade(name=str(i), branch_length=1.0, confidence=1))\n",
    "\n",
    "# 创建树结构\n",
    "tree = Phylo.BaseTree.Tree(root=leaves[0], rooted=False)\n",
    "\n",
    "from Bio.Cluster import treecluster\n",
    "# 添加聚类关系到树中\n",
    "# for i in range(len(X)):\n",
    "#     cluster_label = labels[i]\n",
    "#     tree.clade.clades.append(leaves[i])\n",
    "#     tree.clade.clades[cluster_label].clades.append(leaves[i])\n",
    "\n",
    "# 将树保存为 Newick 文件\n",
    "filename = \"cluster_tree.newick\"\n",
    "Phylo.write(tree, filename, \"newick\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1): 0.666667\n",
      "(-1, 0): 9.66667\n",
      "0\n",
      "1\n",
      "[Clade(name='0'), Clade(name='1')]\n",
      "[2 1 0]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tree' object has no attribute 'tree'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12924/154655547.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# 添加聚类关系到树中\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfor\u001b[0m  \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcluster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0mparent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mleaves\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mchild\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mleaves\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Tree' object has no attribute 'tree'"
     ]
    }
   ],
   "source": [
    "from Bio.Cluster import treecluster\n",
    "\n",
    "import numpy as np\n",
    "distance = np.array([[1,2,3],[4,5,6],[3,5,7]])\n",
    "cluster = treecluster(distance)\n",
    "print(cluster)\n",
    "\n",
    "# 创建叶节点\n",
    "leaves = []\n",
    "for i in range(len(cluster)):\n",
    "    print(i)\n",
    "    leaves.append(Phylo.BaseTree.Clade(name=str(i)))\n",
    "\n",
    "print(leaves)\n",
    "# 创建树结构\n",
    "tree = Phylo.BaseTree.Tree(root=leaves[0], rooted=False)\n",
    "\n",
    "print(cluster.cut())\n",
    "\n",
    "# 添加聚类关系到树中\n",
    "for  c in cluster.tree:\n",
    "    parent = leaves[c[0]]\n",
    "    child = leaves[c[1]]\n",
    "    parent.clades.append(child)\n",
    "\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://codepal.ai/code-generator/query/X8TIT2PW/generate-newick-string-from-distance-matrix\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def neighbor_joining(distance_matrix):\n",
    "    \"\"\"\n",
    "    Generates a Newick string from a distance matrix by performing neighbor joining.\n",
    "\n",
    "    Parameters:\n",
    "    - distance_matrix: numpy.ndarray\n",
    "        A square matrix representing the pairwise distances between the data points.\n",
    "\n",
    "    Returns:\n",
    "    - str:\n",
    "        The Newick string representation of the tree.\n",
    "\n",
    "    Raises:\n",
    "    - ValueError:\n",
    "        Raises an error if the distance matrix is not a square matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    # Checking if the distance matrix is a square matrix\n",
    "    if distance_matrix.shape[0] != distance_matrix.shape[1]:\n",
    "        raise ValueError(\"Distance matrix should be a square matrix.\")\n",
    "\n",
    "    # Number of data points\n",
    "    n = distance_matrix.shape[0]\n",
    "\n",
    "    # Initialize the tree with n leaves\n",
    "    tree = [str(i) for i in range(n)]\n",
    "\n",
    "    while n > 2:\n",
    "        # Calculate the total distance for each data point\n",
    "        total_distance = np.sum(distance_matrix, axis=1)\n",
    "\n",
    "        # Calculate the Q matrix\n",
    "        q_matrix = np.zeros((n, n))\n",
    "        for i in range(n):\n",
    "            for j in range(i+1, n):\n",
    "                q_matrix[i, j] = (n-2) * distance_matrix[i, j] - total_distance[i] - total_distance[j]\n",
    "                q_matrix[j, i] = q_matrix[i, j]\n",
    "\n",
    "        # Find the minimum element in the Q matrix\n",
    "        min_value = np.min(q_matrix[q_matrix > 0])\n",
    "        min_indices = np.where(q_matrix == min_value)\n",
    "\n",
    "        # Get the indices of the two closest data points\n",
    "        i, j = min_indices[0][0], min_indices[1][0]\n",
    "\n",
    "        # Calculate the branch lengths\n",
    "        branch_length_i = 0.5 * distance_matrix[i, j] + (total_distance[i] - total_distance[j]) / (2 * (n - 2))\n",
    "        branch_length_j = distance_matrix[i, j] - branch_length_i\n",
    "\n",
    "        # Create a new node for the two closest data points\n",
    "        new_node = \"(\" + tree[i] + \":\" + str(branch_length_i) + \",\" + tree[j] + \":\" + str(branch_length_j) + \")\"\n",
    "\n",
    "        # Update the tree and distance matrix\n",
    "        tree.append(new_node)\n",
    "        tree.pop(i)\n",
    "        tree.pop(j-1)\n",
    "\n",
    "        distance_matrix = np.delete(distance_matrix, j, axis=0)\n",
    "        distance_matrix = np.delete(distance_matrix, j, axis=1)\n",
    "\n",
    "        for k in range(n-1):\n",
    "            if k != i:\n",
    "                distance_matrix[i, k] = 0.5 * (distance_matrix[i, k] + distance_matrix[j, k] - distance_matrix[i, j])\n",
    "                distance_matrix[k, i] = distance_matrix[i, k]\n",
    "\n",
    "        distance_matrix = np.delete(distance_matrix, j, axis=0)\n",
    "        distance_matrix = np.delete(distance_matrix, j, axis=1)\n",
    "\n",
    "        n -= 1\n",
    "\n",
    "    # Combine the remaining two nodes into a single tree\n",
    "    newick_string = \"(\" + tree[0] + \":\" + str(distance_matrix[0, 1]) + \",\" + tree[1] + \":\" + str(distance_matrix[0, 1]) + \");\"\n",
    "\n",
    "    return newick_string\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "# Distance matrix\n",
    "distance_matrix = np.array([[0, 5, 9, 9],\n",
    "                            [5, 0, 10, 10],\n",
    "                            [9, 10, 0, 8],\n",
    "                            [9, 10, 8, 0]])\n",
    "\n",
    "# Generate Newick string\n",
    "newick = neighbor_joining(distance_matrix)\n",
    "print(\"Newick string:\", newick)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(((((b:1.4933428258506058,a:1.4933428258506058):0.7680867725836196,d:2.2614295984342254):0.22625259477372683,((i:0.6909506384345234,c:0.6909506384345234):0.7527738955234908,f:1.4437245339580143):1.043957659249938):0.5378290788876594,((g:1.3194839206290894,e:1.3194839206290894):0.5976128408576096,h:1.917096761486699):1.1084145106089127):1.5784862028041697,j:4.603997474899781);'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.cluster import hierarchy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import to_tree, ClusterNode, dendrogram\n",
    "from typing import Dict, Tuple, List, Union, Optional\n",
    "from string import ascii_lowercase\n",
    "\n",
    "data1=np.random.normal(size=(10,5)) #随机生成10个样本，5个指标的数据，进行Q型层次聚类\n",
    "labels=np.arange(10) #样本标签\n",
    " \n",
    "plt.figure(figsize=(5,5)) #画布大小\n",
    "Z=hierarchy.linkage(y=data1,method='weighted',metric='euclidean') #生成聚类树\n",
    "# Z.tolist()\n",
    "T = to_tree(Z, rd=False)\n",
    "T\n",
    "\n",
    "# Generating Newick string output from hierarchical clustering of some cgMLST profiles\n",
    "# https://gist.github.com/peterk87/b203f62a71d7f4fb273139b219af5e81\n",
    "def _scipy_tree_to_newick_list(node: ClusterNode, newick: List[str], parentdist: float, leaf_names: List[str]) -> List[str]:\n",
    "    \"\"\"Construct Newick tree from SciPy hierarchical clustering ClusterNode\n",
    "\n",
    "    This is a recursive function to help build a Newick output string from a scipy.cluster.hierarchy.to_tree input with\n",
    "    user specified leaf node names.\n",
    "\n",
    "    Notes:\n",
    "        This function is meant to be used with `to_newick`\n",
    "\n",
    "    Args:\n",
    "        node (scipy.cluster.hierarchy.ClusterNode): Root node is output of scipy.cluster.hierarchy.to_tree from hierarchical clustering linkage matrix\n",
    "        parentdist (float): Distance of parent node of `node`\n",
    "        newick (list of string): Newick string output accumulator list which needs to be reversed and concatenated (i.e. `''.join(newick)`) for final output\n",
    "        leaf_names (list of string): Leaf node names\n",
    "\n",
    "    Returns:\n",
    "        (list of string): Returns `newick` list of Newick output strings\n",
    "    \"\"\"\n",
    "    if node.is_leaf():\n",
    "        return newick + [f'{leaf_names[node.id]}:{parentdist - node.dist}']\n",
    "\n",
    "    if len(newick) > 0:\n",
    "        newick.append(f'):{parentdist - node.dist}')\n",
    "    else:\n",
    "        newick.append(');')\n",
    "    newick = _scipy_tree_to_newick_list(node.get_left(), newick, node.dist, leaf_names)\n",
    "    newick.append(',')\n",
    "    newick = _scipy_tree_to_newick_list(node.get_right(), newick, node.dist, leaf_names)\n",
    "    newick.append('(')\n",
    "    return newick\n",
    "\n",
    "\n",
    "def to_newick(tree: ClusterNode, leaf_names: List[str]) -> str:\n",
    "    \"\"\"Newick tree output string from SciPy hierarchical clustering tree\n",
    "\n",
    "    Convert a SciPy ClusterNode tree to a Newick format string.\n",
    "    Use scipy.cluster.hierarchy.to_tree on a hierarchical clustering linkage matrix to create the root ClusterNode for the `tree` input of this function.\n",
    "\n",
    "    Args:\n",
    "        tree (scipy.cluster.hierarchy.ClusterNode): Output of scipy.cluster.hierarchy.to_tree from hierarchical clustering linkage matrix\n",
    "        leaf_names (list of string): Leaf node names\n",
    "\n",
    "    Returns:\n",
    "        (string): Newick output string\n",
    "    \"\"\"\n",
    "    newick_list = _scipy_tree_to_newick_list(tree, [], tree.dist, leaf_names)\n",
    "    return ''.join(newick_list[::-1])\n",
    "\n",
    "to_newick(T, ascii_lowercase)\n",
    "# hierarchy.dendrogram(Z,labels=labels) #画聚类树\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Cherry\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'c:/Users/Cherry/Desktop/testtutor.m4a'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16800/4199241951.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# 调用函数进行音频剪辑，传入音频文件路径和每段持续时间（分钟）\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0msplit_audio\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"c:/Users/Cherry/Desktop/testtutor.m4a\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16800/4199241951.py\u001b[0m in \u001b[0;36msplit_audio\u001b[1;34m(file_path, segment_duration)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msplit_audio\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msegment_duration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0maudio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAudioSegment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mtotal_duration\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0msegment_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msegment_duration\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m60\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m1000\u001b[0m  \u001b[1;31m# 转换为毫秒\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Cherry\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pydub\\audio_segment.py\u001b[0m in \u001b[0;36mfrom_file\u001b[1;34m(cls, file, format, codec, parameters, start_second, duration, **kwargs)\u001b[0m\n\u001b[0;32m    649\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    650\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 651\u001b[1;33m         \u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclose_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_fd_or_path_or_tempfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtempfile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    652\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    653\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Cherry\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pydub\\utils.py\u001b[0m in \u001b[0;36m_fd_or_path_or_tempfile\u001b[1;34m(fd, mode, tempfile)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m         \u001b[0mfd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m         \u001b[0mclose_fd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'c:/Users/Cherry/Desktop/testtutor.m4a'"
     ]
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "import math\n",
    "\n",
    "def split_audio(file_path, segment_duration):\n",
    "    audio = AudioSegment.from_file(file_path)\n",
    "    total_duration = len(audio)\n",
    "    segment_length = segment_duration * 60 * 1000  # 转换为毫秒\n",
    "\n",
    "    num_segments = math.ceil(total_duration / segment_length)\n",
    "\n",
    "    for i in range(num_segments):\n",
    "        start_time = i * segment_length\n",
    "        end_time = start_time + segment_length\n",
    "        segment = audio[start_time:end_time]\n",
    "\n",
    "        # 保存每个片段\n",
    "        segment.export(f\"segment_{i+1}.mp3\", format=\"mp3\")\n",
    "\n",
    "# 调用函数进行音频剪辑，传入音频文件路径和每段持续时间（分钟）\n",
    "split_audio(f\"c:/Users/Cherry/Desktop/testtutor.m4a\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "tags": [
     "data"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6 , 0.25, 0.  , 0.7 ],\n",
       "       [0.1 , 0.25, 0.  , 0.5 ],\n",
       "       [0.3 , 0.  , 0.  , 0.3 ],\n",
       "       [0.2 , 0.5 , 0.  , 0.2 ],\n",
       "       [0.7 , 0.25, 0.  , 0.6 ],\n",
       "       [1.  , 1.  , 1.  , 1.  ],\n",
       "       [0.5 , 0.25, 0.5 , 0.2 ],\n",
       "       [0.5 , 0.5 , 0.  , 0.6 ],\n",
       "       [0.  , 0.25, 0.  , 0.  ]])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "# 1. 首先判断data是否是空值\n",
    "# min-max 归一化\n",
    "\n",
    "import math\n",
    "import csv\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "# from scipy.stats import minmax_scale\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "\n",
    "# https://www.omicshare.com/tools/Home/Soft/normalization\n",
    "# https://cloud.tencent.com/developer/article/1387731\n",
    "# https://blog.sciencenet.cn/home.php?mod=space&uid=3431904&do=blog&id=1274291\n",
    "# with open('C:/Users/Cherry/Desktop/geekbang/bioinformatics/learnD3/Iris1.txt', newline = '') as csvfile:\n",
    "#     spamreader = csv.reader(csvfile, delimiter='/t')\n",
    "#     for row in spamreader:\n",
    "#         print(row)\n",
    "\t\t# print(', '.join(row))\n",
    "\n",
    "filepath = f'C:/Users/Cherry/Downloads/normalization_or_standardizatation_input_file.txt'\n",
    "# 读取csv文件\n",
    "# 指定第一列作为索引\n",
    "df = pd.read_table(filepath, sep = \"/t\", index_col = 0, header = 0)\n",
    "\n",
    "# print(df)\n",
    "# 提取整个DataFrame\n",
    "data = df\n",
    "\n",
    "\n",
    "# 1. 0-1 归一化\n",
    "def minmax_normalize(data):\n",
    "\tscaler = MinMaxScaler()\n",
    "\tnormalized_data = scaler.fit_transform(data)\n",
    "\treturn normalized_data\n",
    "\n",
    "# 2. z-socre 归一化, 变成正态分布\n",
    "def zscore_normalize(data):\n",
    "\tscaler = StandardScaler()\n",
    "\treturn scaler.fit_transform(data)\n",
    "\n",
    "# 3.log 归一化\n",
    "def log_transform(data):\n",
    "\treturn np.log1p(data)\n",
    "\n",
    "def log2_transform(data):\n",
    "\treturn np.log2(data)\n",
    "\n",
    "def log10_transform(data):\n",
    "\treturn np.log10(data)\n",
    "\n",
    "# 4. mean 归一化\n",
    "# df = pd.DataFrame(np.random.randint(-100, 100, size=(20, 4)), columns=list(\"ABCD\"))\n",
    "def mean_norm(df):\n",
    "    return (df - df.mean()) / (df.max() - df.min())\n",
    "\n",
    "# 5. center 中心值归一化\n",
    "def center_normalize(data):\n",
    "\tmean = np.mean(data)\n",
    "\treturn (data - mean)\n",
    "\n",
    "\n",
    "# 6. 帕莱托分布 归一化\n",
    "# TODO:xiaojiao, 寻找z-score 和 帕莱托归一化的区别\n",
    "def pareto_norm(data):\n",
    "    mean = data.mean()\n",
    "    std = data.std()\n",
    "\n",
    "    normalized = (data - mean) / std\n",
    "\t# normalized = (data - min_) / (max_ - min_)\n",
    "\t# normalized = 1 - (1/normalized)**scale\n",
    "    return normalized\n",
    "\n",
    "\n",
    "normalized_data = minmax_normalize(data)\n",
    "\n",
    "# minmax_normalize(data)\n",
    "# 对DataFrame进行归一化\n",
    "# print(data.min(), data.max(), data.mean(),\n",
    "#       data.std())\n",
    "normalized_data\n",
    "\n",
    "# from scipy import stats\n",
    "# normalized_data = stats.zscore(data)\n",
    "normalized_data\n",
    "# 生成新的DataFrame\n",
    "# normalized_df = pd.DataFrame(normalized_data, columns=df.columns)\n",
    "\n",
    "# normalized_df\n",
    "# print(normalized_df)\n",
    "# 写回csv文件\n",
    "# normalized_df.to_csv('C:/Users/Cherry/Desktop/geekbang/bioinformatics/learnD3/normalized_data.csv', index=False)\n",
    "\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# matrix = np.array([ [1, 0.8210576858123295, 0.8705892711689287, 0.824566479341442, 0.8541313718020874, 0.7398956945034544, 0.4780323726958185, 0.8280752728705546, 0.9193039046274801, 0.7929873375794294, 0.7684257828756417, 0.8014072130488722, 0.8386016534578921], \n",
    "#                     [0.0, 1.0, 0.9329146104170898, 0.8811188811188813, 0.833626497322632, 0.8126107032724816, 0.6409817185295869, 0.7762237762237763, 0.8951048951048951, 0.8811188811188813, 0.8741258741258742, 0.8056054385890983, 0.8181818181818183], [0.0, 0.0, 1.0, 0.830435353969758, 0.812421193401995, 0.7982613468938993, 0.4708148963941845, 0.6926184228854153, 0.9117120056348833, 0.8056989817238502, 0.7668275396231382, 0.757551788183124, 0.8198340515786545], [0.0, 0.0, 0.0, 1.0, 0.9106844088398501, 0.9422080999150758, 0.7075317330217298, 0.8181818181818183, 0.8741258741258742, 0.9510489510489512, 0.8951048951048951, 0.7145369977051133, 0.7342657342657343], [0.0, 0.0, 0.0, 0.0, 1.0, 0.8596491228070177, 0.775438596491228, 0.6795106742881959, 0.9281975705483089, 0.8511396590310907, 0.9071817764981586, 0.636842105263158, 0.7075317330217298], [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5982456140350878, 0.6514896155546621, 0.8266212326392487, 0.8931712471313915, 0.7635738504887974, 0.5508771929824562, 0.6409817185295869], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.4903685278368424, 0.612960659796053, 0.5919448657459025, 0.8021028062474065, 0.3087719298245614, 0.3047290137271806], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6923076923076924, 0.8391608391608393, 0.7552447552447553, 0.8616475560561659, 0.7762237762237763], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.8181818181818183, 0.8601398601398602, 0.7390554240969553, 0.8041958041958043], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.8461538461538463, 0.8021028062474066, 0.8321678321678322], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.7250448947301884, 0.6643356643356644], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.9422080999150758], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]])\n",
    "\n",
    "# transposed_matrix = matrix.T\n",
    "# print(transposed_matrix.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Iris_setosa ', 5.1)\n",
      "('Iris_versicolor ', 7.0)\n",
      "('Iris_virginica', 6.3)\n",
      "('Iris_setosa ', 4.9)\n",
      "('Iris_versicolor ', 6.4)\n",
      "('Iris_virginica', 5.8)\n",
      "('Iris_setosa ', 4.7)\n",
      "('Iris_versicolor ', 6.9)\n",
      "('Iris_virginica', 7.1)\n",
      "('Iris_setosa ', 4.6)\n",
      "('Iris_versicolor ', 5.5)\n",
      "('Iris_virginica', 6.3)\n",
      "('Iris_setosa ', 5.0)\n",
      "('Iris_versicolor ', 6.5)\n",
      "('Iris_virginica', 6.5)\n",
      "('Iris_setosa ', 5.4)\n",
      "('Iris_versicolor ', 5.7)\n",
      "('Iris_virginica', 7.6)\n",
      "('Iris_setosa ', 4.6)\n",
      "('Iris_versicolor ', 6.3)\n",
      "('Iris_virginica', 4.9)\n",
      "('Iris_setosa ', 5.0)\n",
      "('Iris_versicolor ', 4.9)\n",
      "('Iris_virginica', 7.3)\n",
      "('Iris_setosa ', 4.4)\n",
      "('Iris_versicolor ', 6.6)\n",
      "('Iris_virginica', 6.7)\n",
      "('Iris_setosa ', 4.9)\n",
      "('Iris_versicolor ', 5.2)\n",
      "('Iris_virginica', 7.2)\n",
      "('Iris_setosa ', 5.4)\n",
      "('Iris_versicolor ', 5.0)\n",
      "('Iris_virginica', 6.5)\n",
      "('Iris_setosa ', 4.8)\n",
      "('Iris_versicolor ', 5.9)\n",
      "('Iris_virginica', 6.4)\n",
      "('Iris_setosa ', 4.8)\n",
      "('Iris_versicolor ', 6.0)\n",
      "('Iris_virginica', 6.8)\n",
      "('Iris_setosa ', 4.3)\n",
      "('Iris_versicolor ', 6.1)\n",
      "('Iris_virginica', 5.7)\n",
      "('Iris_setosa ', 5.8)\n",
      "('Iris_versicolor ', 5.6)\n",
      "('Iris_virginica', 5.8)\n",
      "('Iris_setosa ', 5.7)\n",
      "('Iris_versicolor ', 6.7)\n",
      "('Iris_virginica', 6.4)\n",
      "('Iris_setosa ', 5.4)\n",
      "('Iris_versicolor ', 5.6)\n",
      "('Iris_virginica', 6.5)\n",
      "('Iris_setosa ', 5.1)\n",
      "('Iris_versicolor ', 5.8)\n",
      "('Iris_virginica', 7.7)\n",
      "('Iris_setosa ', 5.7)\n",
      "('Iris_versicolor ', 6.2)\n",
      "('Iris_virginica', 7.7)\n",
      "('Iris_setosa ', 5.1)\n",
      "('Iris_versicolor ', 5.6)\n",
      "('Iris_virginica', 6.0)\n",
      "('Iris_setosa ', 5.4)\n",
      "('Iris_versicolor ', 5.9)\n",
      "('Iris_virginica', 6.9)\n",
      "('Iris_setosa ', 5.1)\n",
      "('Iris_versicolor ', 6.1)\n",
      "('Iris_virginica', 5.6)\n",
      "('Iris_setosa ', 4.6)\n",
      "('Iris_versicolor ', 6.3)\n",
      "('Iris_virginica', 7.7)\n",
      "('Iris_setosa ', 5.1)\n",
      "('Iris_versicolor ', 6.1)\n",
      "('Iris_virginica', 6.3)\n",
      "('Iris_setosa ', 4.8)\n",
      "('Iris_versicolor ', 6.4)\n",
      "('Iris_virginica', 6.7)\n",
      "('Iris_setosa ', 5.0)\n",
      "('Iris_versicolor ', 6.6)\n",
      "('Iris_virginica', 7.2)\n",
      "('Iris_setosa ', 5.0)\n",
      "('Iris_versicolor ', 6.8)\n",
      "('Iris_virginica', 6.2)\n",
      "('Iris_setosa ', 5.2)\n",
      "('Iris_versicolor ', 6.7)\n",
      "('Iris_virginica', 6.1)\n",
      "('Iris_setosa ', 5.2)\n",
      "('Iris_versicolor ', 6.0)\n",
      "('Iris_virginica', 6.4)\n",
      "('Iris_setosa ', 4.7)\n",
      "('Iris_versicolor ', 5.7)\n",
      "('Iris_virginica', 7.2)\n",
      "('Iris_setosa ', 4.8)\n",
      "('Iris_versicolor ', 5.5)\n",
      "('Iris_virginica', 7.4)\n",
      "('Iris_setosa ', 5.4)\n",
      "('Iris_versicolor ', 5.5)\n",
      "('Iris_virginica', 7.9)\n",
      "('Iris_setosa ', 5.2)\n",
      "('Iris_versicolor ', 5.8)\n",
      "('Iris_virginica', 6.4)\n",
      "('Iris_setosa ', 5.5)\n",
      "('Iris_versicolor ', 6.0)\n",
      "('Iris_virginica', 6.3)\n",
      "('Iris_setosa ', 4.9)\n",
      "('Iris_versicolor ', 5.4)\n",
      "('Iris_virginica', 6.1)\n",
      "('Iris_setosa ', 5.0)\n",
      "('Iris_versicolor ', 6.0)\n",
      "('Iris_virginica', 7.7)\n",
      "('Iris_setosa ', 5.5)\n",
      "('Iris_versicolor ', 6.7)\n",
      "('Iris_virginica', 6.3)\n",
      "('Iris_setosa ', 4.9)\n",
      "('Iris_versicolor ', 6.3)\n",
      "('Iris_virginica', 6.4)\n",
      "('Iris_setosa ', 4.4)\n",
      "('Iris_versicolor ', 5.6)\n",
      "('Iris_virginica', 6.0)\n",
      "('Iris_setosa ', 5.1)\n",
      "('Iris_versicolor ', 5.5)\n",
      "('Iris_virginica', 6.9)\n",
      "('Iris_setosa ', 5.0)\n",
      "('Iris_versicolor ', 5.5)\n",
      "('Iris_virginica', 6.7)\n",
      "('Iris_setosa ', 4.5)\n",
      "('Iris_versicolor ', 6.1)\n",
      "('Iris_virginica', 6.9)\n",
      "('Iris_setosa ', 4.4)\n",
      "('Iris_versicolor ', 5.8)\n",
      "('Iris_virginica', 5.8)\n",
      "('Iris_setosa ', 5.0)\n",
      "('Iris_versicolor ', 5.0)\n",
      "('Iris_virginica', 6.8)\n",
      "('Iris_setosa ', 5.1)\n",
      "('Iris_versicolor ', 5.6)\n",
      "('Iris_virginica', 6.7)\n",
      "('Iris_setosa ', 4.8)\n",
      "('Iris_versicolor ', 5.7)\n",
      "('Iris_virginica', 6.7)\n",
      "('Iris_setosa ', 5.1)\n",
      "('Iris_versicolor ', 5.7)\n",
      "('Iris_virginica', 6.3)\n",
      "('Iris_setosa ', 4.6)\n",
      "('Iris_versicolor ', 6.2)\n",
      "('Iris_virginica', 6.5)\n",
      "('Iris_setosa ', 5.3)\n",
      "('Iris_versicolor ', 5.1)\n",
      "('Iris_virginica', 6.2)\n",
      "('Iris_setosa ', 5.0)\n",
      "('Iris_versicolor ', 5.7)\n",
      "('Iris_virginica', 5.9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Cherry\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\core\\frame.py:1554: FutureWarning: Using short name for 'orient' is deprecated. Only the options: ('dict', list, 'series', 'split', 'records', 'index') will be used in a future version. Use one of the above to silence this warning.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'sepecies': 'Iris_setosa ', 'value': 5.1},\n",
       " {'sepecies': 'Iris_versicolor ', 'value': 7.0},\n",
       " {'sepecies': 'Iris_virginica', 'value': 6.3},\n",
       " {'sepecies': 'Iris_setosa ', 'value': 4.9},\n",
       " {'sepecies': 'Iris_versicolor ', 'value': 6.4},\n",
       " {'sepecies': 'Iris_virginica', 'value': 5.8},\n",
       " {'sepecies': 'Iris_setosa ', 'value': 4.7},\n",
       " {'sepecies': 'Iris_versicolor ', 'value': 6.9},\n",
       " {'sepecies': 'Iris_virginica', 'value': 7.1},\n",
       " {'sepecies': 'Iris_setosa ', 'value': 4.6},\n",
       " {'sepecies': 'Iris_versicolor ', 'value': 5.5},\n",
       " {'sepecies': 'Iris_virginica', 'value': 6.3},\n",
       " {'sepecies': 'Iris_setosa ', 'value': 5.0},\n",
       " {'sepecies': 'Iris_versicolor ', 'value': 6.5},\n",
       " {'sepecies': 'Iris_virginica', 'value': 6.5},\n",
       " {'sepecies': 'Iris_setosa ', 'value': 5.4},\n",
       " {'sepecies': 'Iris_versicolor ', 'value': 5.7},\n",
       " {'sepecies': 'Iris_virginica', 'value': 7.6},\n",
       " {'sepecies': 'Iris_setosa ', 'value': 4.6},\n",
       " {'sepecies': 'Iris_versicolor ', 'value': 6.3},\n",
       " {'sepecies': 'Iris_virginica', 'value': 4.9},\n",
       " {'sepecies': 'Iris_setosa ', 'value': 5.0},\n",
       " {'sepecies': 'Iris_versicolor ', 'value': 4.9},\n",
       " {'sepecies': 'Iris_virginica', 'value': 7.3},\n",
       " {'sepecies': 'Iris_setosa ', 'value': 4.4},\n",
       " {'sepecies': 'Iris_versicolor ', 'value': 6.6},\n",
       " {'sepecies': 'Iris_virginica', 'value': 6.7},\n",
       " {'sepecies': 'Iris_setosa ', 'value': 4.9},\n",
       " {'sepecies': 'Iris_versicolor ', 'value': 5.2},\n",
       " {'sepecies': 'Iris_virginica', 'value': 7.2},\n",
       " {'sepecies': 'Iris_setosa ', 'value': 5.4},\n",
       " {'sepecies': 'Iris_versicolor ', 'value': 5.0},\n",
       " {'sepecies': 'Iris_virginica', 'value': 6.5},\n",
       " {'sepecies': 'Iris_setosa ', 'value': 4.8},\n",
       " {'sepecies': 'Iris_versicolor ', 'value': 5.9},\n",
       " {'sepecies': 'Iris_virginica', 'value': 6.4},\n",
       " {'sepecies': 'Iris_setosa ', 'value': 4.8},\n",
       " {'sepecies': 'Iris_versicolor ', 'value': 6.0},\n",
       " {'sepecies': 'Iris_virginica', 'value': 6.8},\n",
       " {'sepecies': 'Iris_setosa ', 'value': 4.3},\n",
       " {'sepecies': 'Iris_versicolor ', 'value': 6.1},\n",
       " {'sepecies': 'Iris_virginica', 'value': 5.7},\n",
       " {'sepecies': 'Iris_setosa ', 'value': 5.8},\n",
       " {'sepecies': 'Iris_versicolor ', 'value': 5.6},\n",
       " {'sepecies': 'Iris_virginica', 'value': 5.8},\n",
       " {'sepecies': 'Iris_setosa ', 'value': 5.7},\n",
       " {'sepecies': 'Iris_versicolor ', 'value': 6.7},\n",
       " {'sepecies': 'Iris_virginica', 'value': 6.4},\n",
       " {'sepecies': 'Iris_setosa ', 'value': 5.4},\n",
       " {'sepecies': 'Iris_versicolor ', 'value': 5.6},\n",
       " {'sepecies': 'Iris_virginica', 'value': 6.5},\n",
       " {'sepecies': 'Iris_setosa ', 'value': 5.1},\n",
       " {'sepecies': 'Iris_versicolor ', 'value': 5.8},\n",
       " {'sepecies': 'Iris_virginica', 'value': 7.7},\n",
       " {'sepecies': 'Iris_setosa ', 'value': 5.7},\n",
       " {'sepecies': 'Iris_versicolor ', 'value': 6.2},\n",
       " {'sepecies': 'Iris_virginica', 'value': 7.7},\n",
       " {'sepecies': 'Iris_setosa ', 'value': 5.1},\n",
       " {'sepecies': 'Iris_versicolor ', 'value': 5.6},\n",
       " {'sepecies': 'Iris_virginica', 'value': 6.0},\n",
       " {'sepecies': 'Iris_setosa ', 'value': 5.4},\n",
       " {'sepecies': 'Iris_versicolor ', 'value': 5.9},\n",
       " {'sepecies': 'Iris_virginica', 'value': 6.9},\n",
       " {'sepecies': 'Iris_setosa ', 'value': 5.1},\n",
       " {'sepecies': 'Iris_versicolor ', 'value': 6.1},\n",
       " {'sepecies': 'Iris_virginica', 'value': 5.6},\n",
       " {'sepecies': 'Iris_setosa ', 'value': 4.6},\n",
       " {'sepecies': 'Iris_versicolor ', 'value': 6.3},\n",
       " {'sepecies': 'Iris_virginica', 'value': 7.7},\n",
       " {'sepecies': 'Iris_setosa ', 'value': 5.1},\n",
       " {'sepecies': 'Iris_versicolor ', 'value': 6.1},\n",
       " {'sepecies': 'Iris_virginica', 'value': 6.3},\n",
       " {'sepecies': 'Iris_setosa ', 'value': 4.8},\n",
       " {'sepecies': 'Iris_versicolor ', 'value': 6.4},\n",
       " {'sepecies': 'Iris_virginica', 'value': 6.7},\n",
       " {'sepecies': 'Iris_setosa ', 'value': 5.0},\n",
       " {'sepecies': 'Iris_versicolor ', 'value': 6.6},\n",
       " {'sepecies': 'Iris_virginica', 'value': 7.2},\n",
       " {'sepecies': 'Iris_setosa ', 'value': 5.0},\n",
       " {'sepecies': 'Iris_versicolor ', 'value': 6.8},\n",
       " {'sepecies': 'Iris_virginica', 'value': 6.2},\n",
       " {'sepecies': 'Iris_setosa ', 'value': 5.2},\n",
       " {'sepecies': 'Iris_versicolor ', 'value': 6.7},\n",
       " {'sepecies': 'Iris_virginica', 'value': 6.1},\n",
       " {'sepecies': 'Iris_setosa ', 'value': 5.2},\n",
       " {'sepecies': 'Iris_versicolor ', 'value': 6.0},\n",
       " {'sepecies': 'Iris_virginica', 'value': 6.4},\n",
       " {'sepecies': 'Iris_setosa ', 'value': 4.7},\n",
       " {'sepecies': 'Iris_versicolor ', 'value': 5.7},\n",
       " {'sepecies': 'Iris_virginica', 'value': 7.2},\n",
       " {'sepecies': 'Iris_setosa ', 'value': 4.8},\n",
       " {'sepecies': 'Iris_versicolor ', 'value': 5.5},\n",
       " {'sepecies': 'Iris_virginica', 'value': 7.4},\n",
       " {'sepecies': 'Iris_setosa ', 'value': 5.4},\n",
       " {'sepecies': 'Iris_versicolor ', 'value': 5.5},\n",
       " {'sepecies': 'Iris_virginica', 'value': 7.9},\n",
       " {'sepecies': 'Iris_setosa ', 'value': 5.2},\n",
       " {'sepecies': 'Iris_versicolor ', 'value': 5.8},\n",
       " {'sepecies': 'Iris_virginica', 'value': 6.4},\n",
       " {'sepecies': 'Iris_setosa ', 'value': 5.5},\n",
       " {'sepecies': 'Iris_versicolor ', 'value': 6.0},\n",
       " {'sepecies': 'Iris_virginica', 'value': 6.3},\n",
       " {'sepecies': 'Iris_setosa ', 'value': 4.9},\n",
       " {'sepecies': 'Iris_versicolor ', 'value': 5.4},\n",
       " {'sepecies': 'Iris_virginica', 'value': 6.1},\n",
       " {'sepecies': 'Iris_setosa ', 'value': 5.0},\n",
       " {'sepecies': 'Iris_versicolor ', 'value': 6.0},\n",
       " {'sepecies': 'Iris_virginica', 'value': 7.7},\n",
       " {'sepecies': 'Iris_setosa ', 'value': 5.5},\n",
       " {'sepecies': 'Iris_versicolor ', 'value': 6.7},\n",
       " {'sepecies': 'Iris_virginica', 'value': 6.3},\n",
       " {'sepecies': 'Iris_setosa ', 'value': 4.9},\n",
       " {'sepecies': 'Iris_versicolor ', 'value': 6.3},\n",
       " {'sepecies': 'Iris_virginica', 'value': 6.4},\n",
       " {'sepecies': 'Iris_setosa ', 'value': 4.4},\n",
       " {'sepecies': 'Iris_versicolor ', 'value': 5.6},\n",
       " {'sepecies': 'Iris_virginica', 'value': 6.0},\n",
       " {'sepecies': 'Iris_setosa ', 'value': 5.1},\n",
       " {'sepecies': 'Iris_versicolor ', 'value': 5.5},\n",
       " {'sepecies': 'Iris_virginica', 'value': 6.9},\n",
       " {'sepecies': 'Iris_setosa ', 'value': 5.0},\n",
       " {'sepecies': 'Iris_versicolor ', 'value': 5.5},\n",
       " {'sepecies': 'Iris_virginica', 'value': 6.7},\n",
       " {'sepecies': 'Iris_setosa ', 'value': 4.5},\n",
       " {'sepecies': 'Iris_versicolor ', 'value': 6.1},\n",
       " {'sepecies': 'Iris_virginica', 'value': 6.9},\n",
       " {'sepecies': 'Iris_setosa ', 'value': 4.4},\n",
       " {'sepecies': 'Iris_versicolor ', 'value': 5.8},\n",
       " {'sepecies': 'Iris_virginica', 'value': 5.8},\n",
       " {'sepecies': 'Iris_setosa ', 'value': 5.0},\n",
       " {'sepecies': 'Iris_versicolor ', 'value': 5.0},\n",
       " {'sepecies': 'Iris_virginica', 'value': 6.8},\n",
       " {'sepecies': 'Iris_setosa ', 'value': 5.1},\n",
       " {'sepecies': 'Iris_versicolor ', 'value': 5.6},\n",
       " {'sepecies': 'Iris_virginica', 'value': 6.7},\n",
       " {'sepecies': 'Iris_setosa ', 'value': 4.8},\n",
       " {'sepecies': 'Iris_versicolor ', 'value': 5.7},\n",
       " {'sepecies': 'Iris_virginica', 'value': 6.7},\n",
       " {'sepecies': 'Iris_setosa ', 'value': 5.1},\n",
       " {'sepecies': 'Iris_versicolor ', 'value': 5.7},\n",
       " {'sepecies': 'Iris_virginica', 'value': 6.3},\n",
       " {'sepecies': 'Iris_setosa ', 'value': 4.6},\n",
       " {'sepecies': 'Iris_versicolor ', 'value': 6.2},\n",
       " {'sepecies': 'Iris_virginica', 'value': 6.5},\n",
       " {'sepecies': 'Iris_setosa ', 'value': 5.3},\n",
       " {'sepecies': 'Iris_versicolor ', 'value': 5.1},\n",
       " {'sepecies': 'Iris_virginica', 'value': 6.2},\n",
       " {'sepecies': 'Iris_setosa ', 'value': 5.0},\n",
       " {'sepecies': 'Iris_versicolor ', 'value': 5.7},\n",
       " {'sepecies': 'Iris_virginica', 'value': 5.9}]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_table(\"C:/Users/Cherry/Desktop/geekbang/bioinformatics/learnD3/dynamic_raincloud_chart_inputFile_with_multiple_columns.txt\",\n",
    "                    #  index_col=1,\n",
    "                     header=0)\n",
    "\n",
    "data = data.drop(columns=data.columns.tolist()[0])\n",
    "columns = data.columns.tolist()\n",
    "data.T\n",
    "temp = data.T\n",
    "\n",
    "temp_result = []\n",
    "temp.columns\n",
    "# pd.DataFrame.iloc\n",
    "for i in temp.columns.to_list():\n",
    "    result = temp.iloc[:, i]\n",
    "    # print(result, \">>> \")\n",
    "    for row in result.to_dict().items():\n",
    "        print(row)\n",
    "        # print(row,  result.to_dict())\n",
    "        temp_result.append(row)\n",
    "    # print(result.to_string())\n",
    "    # temp_result.append(result.to_json())\n",
    "\n",
    "# print(temp_result)\n",
    "\n",
    "\n",
    "temp_result2 = pd.DataFrame(temp_result, columns=[\"sepecies\", \"value\"])\n",
    "temp_result2.to_dict(\"record\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sucess\n"
     ]
    },
    {
     "ename": "StatisticsError",
     "evalue": "variance requires at least two data points",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStatisticsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16800/3104524597.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    704\u001b[0m                 \u001b[1;31m# \"C:/Users/Cherry/Desktop/geekbang/bioinformatics/learnD3/\",\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    705\u001b[0m                 \u001b[0mcontained_name_with_colname_rowname\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34m\"都有\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 706\u001b[1;33m                 parametric_method=\"min-max\")\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16800/3104524597.py\u001b[0m in \u001b[0;36mbatch_effect_rectification2\u001b[1;34m(input_file, contained_name_with_colname_rowname, parametric_method)\u001b[0m\n\u001b[0;32m    637\u001b[0m                  if is_number(x)]\n\u001b[0;32m    638\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 639\u001b[1;33m     \u001b[0mdata_std\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstatistics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdev\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    640\u001b[0m     \u001b[0mdata_mean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstatistics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m     \u001b[0mdata_variance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstatistics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Cherry\\AppData\\Local\\Programs\\Python\\Python37\\lib\\statistics.py\u001b[0m in \u001b[0;36mstdev\u001b[1;34m(data, xbar)\u001b[0m\n\u001b[0;32m    647\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m     \"\"\"\n\u001b[1;32m--> 649\u001b[1;33m     \u001b[0mvar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvariance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxbar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    650\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    651\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Cherry\\AppData\\Local\\Programs\\Python\\Python37\\lib\\statistics.py\u001b[0m in \u001b[0;36mvariance\u001b[1;34m(data, xbar)\u001b[0m\n\u001b[0;32m    585\u001b[0m     \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    586\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 587\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mStatisticsError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'variance requires at least two data points'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    588\u001b[0m     \u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_ss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxbar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mss\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mStatisticsError\u001b[0m: variance requires at least two data points"
     ]
    }
   ],
   "source": [
    "# !/usr/bin/python3\n",
    "# --- encoding: utf-8 ---\n",
    "import statistics\n",
    "import math\n",
    "import csv\n",
    "from abc import ABC, abstractmethod\n",
    "import random\n",
    "import subprocess\n",
    "import csv\n",
    "from functools import partial\n",
    "\n",
    "# 参考资料\n",
    "# https://blog.csdn.net/weixin_46713695/article/details/126301991\n",
    "def get_nested_list_mean(df1_data, flag=True):\n",
    "    len_df1 = len(df1_data)\n",
    "    cols = len(df1_data[0]) - 1\n",
    "    means = [None] * cols\n",
    "    index  = 0\n",
    "    # for i in range(len_df1):\n",
    "        # col = [row[i] for row in df1_data]\n",
    "    for col in df1_data:\n",
    "        if len(col) != len_df1:\n",
    "            pass\n",
    "        # 如果第一列有列名，去除列名\n",
    "        if flag:\n",
    "            col.pop(0)\n",
    "        print(\"col >>\", col)\n",
    "        col_with_number = list(map(eval, col))\n",
    "        # col_with_number.pop(0)\n",
    "        \n",
    "        print(col_with_number)  \n",
    "        if len(col_with_number) > 0:  \n",
    "            mean = statistics.mean(col_with_number)  \n",
    "            means[index] = mean\n",
    "            index += 1\n",
    "    \n",
    "    # 去除空值\n",
    "    means = [x for x in means if x]\n",
    "    return statistics.mean(means)\n",
    "\n",
    "\n",
    "\n",
    "def get_nested_list_std(df1_data, flag=True):\n",
    "    len_df1 = len(df1_data)\n",
    "    cols = len(df1_data[0]) - 1\n",
    "    means = [None] * len_df1\n",
    "    index  = 0\n",
    "    # for i in range(len_df1):\n",
    "        # col = [row[i] for row in df1_data]\n",
    "    for col in df1_data:\n",
    "        if len(col) != len_df1:\n",
    "            pass\n",
    "        # 如果第一列有列名，去除列名\n",
    "        if flag:\n",
    "            pass\n",
    "    statistics.stdev(col)\n",
    "\n",
    "\n",
    "# 原文链接：https://blog.csdn.net/weixin_42201701/article/details/103408370\n",
    "def IntervalMappingAfterNormalization(data, data_min, data_max, MIN, MAX):\n",
    "    \"\"\"\n",
    "    归一化映射到任意区间\n",
    "    :param data: 数据 matrix\n",
    "    :param MIN: 目标数据最小值\n",
    "    :param MAX: 目标数据最小值\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \n",
    "    # 当前数据最大值\n",
    "    d_min = data_min\n",
    "    # 当前数据最小值 \n",
    "    d_max = data_max   \n",
    "    \n",
    "    len_df1 = len(data)\n",
    "    cols = len(data[0]) - 1\n",
    "    means = [None] * cols\n",
    "    index  = 0\n",
    "    # for i in range(len_df1):\n",
    "        # col = [row[i] for row in df1_data]\n",
    "    colnames = None\n",
    "    for col in data:\n",
    "        if len(col) != len_df1:\n",
    "            pass\n",
    "        # 如果第一列有列名，去除列名\n",
    "        if True:\n",
    "            colnames = col.pop(0)\n",
    "        # print(\"col >>\", col)\n",
    "        col_with_number = list(map(eval, col))\n",
    "        # col_with_number.pop(0)\n",
    "        \n",
    "        # print(col_with_number)  \n",
    "\n",
    "    \n",
    "        # 去除空值\n",
    "        mean = [MIN + (MAX - MIN) / (d_max - d_min) * (x - d_min) for x in col_with_number if x]\n",
    "        means[index] = mean\n",
    "        index += 1\n",
    "    # return statistics.mean(means)\n",
    "    # return MIN + (MAX - MIN) / (d_max - d_min) * (data - d_min)\n",
    "    return means\n",
    "\n",
    "def interval_mapping_normalization_with_nested_matrix(data, data_min, data_max, MIN, MAX):\n",
    "    \"\"\"\n",
    "    归一化映射到任意区间\n",
    "    :param data: 数据 matrix\n",
    "    :param MIN: 目标数据最小值\n",
    "    :param MAX: 目标数据最小值\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \n",
    "    # 当前数据最大值\n",
    "    d_min = data_min\n",
    "    # 当前数据最小值 \n",
    "    d_max = data_max   \n",
    "    \n",
    "    len_df1 = len(data)\n",
    "    cols = len(data[0]) - 1\n",
    "    means = [None] * len_df1\n",
    "    index  = 0\n",
    "    # for i in range(len_df1):\n",
    "        # col = [row[i] for row in df1_data]\n",
    "    colnames = None\n",
    "    for row in data:\n",
    "        if len(row) != len_df1:\n",
    "            pass\n",
    "        # 判断是否空行, None等等\n",
    "        if len(row) == 0:\n",
    "            continue\n",
    "        # 如果第一列有列名，去除列名\n",
    "        # if False:\n",
    "        #     colnames = col.pop(0)\n",
    "        # print(\"col >>\", col)\n",
    "        # col_with_number = list(map(eval, col))\n",
    "        # col_with_number.pop(0)\n",
    "        \n",
    "        # print(col_with_number)  \n",
    "\n",
    "    \n",
    "        # 去除空值\n",
    "        mean = [MIN + (MAX - MIN) / (d_max - d_min) * (x - d_min) for x in row if x]\n",
    "        means[index] = mean\n",
    "        index += 1\n",
    "    # return statistics.mean(means)\n",
    "    # return MIN + (MAX - MIN) / (d_max - d_min) * (data - d_min)\n",
    "    means = [x for x in means if x is not None]\n",
    "    return means\n",
    "             \n",
    "\n",
    "\n",
    "# data的数据性描述\n",
    "class DataWithDescription:\n",
    "    \"\"\"\n",
    "    嵌合阿拉伯数字矩阵data\n",
    "    \n",
    "    data的数据性描述\n",
    "    \"\"\"\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.data_min = None\n",
    "        self.data_max = None\n",
    "        self.data_mean = None\n",
    "        self.data_std = None\n",
    "        self.data_variance = None\n",
    "        self.data_median = None\n",
    "        self.data_skew = None\n",
    "        self.data_kurtosis = None\n",
    "        self.data_var = None\n",
    "        self.data_describe = None\n",
    "        self.data_range = None\n",
    "        self.data_quantile = None\n",
    "        self.data_percentile = None\n",
    "        self.data_mode = None\n",
    "        self.data_unique = None\n",
    "        self.data_count = None\n",
    "        self.data_count_distinct = None\n",
    "        self.data_count_null = None\n",
    "        self.data_count_not_null = None\n",
    "    \n",
    "    def iter_data(self):\n",
    "        pass\n",
    "\n",
    "class Normalization(ABC):\n",
    "    @abstractmethod\n",
    "    def transform(self, data):\n",
    "        pass\n",
    "\n",
    "# #  MinMaxNormalizer类\n",
    "# # 0 - 1 归一化\n",
    "class MinMaxNormalizer(Normalization):\n",
    "\n",
    "    def transform(self, data):\n",
    "        # 注意传入的是一个data对象\n",
    "        data_input = data.data\n",
    "        data_max = data.data_max\n",
    "        data_min = data.data_min\n",
    "        # print(self.data)\n",
    "        print(\"data description >>>\", data_max, data_min)\n",
    "        result = interval_mapping_normalization_with_nested_matrix(data_input, \n",
    "                                                          data_max, data_min,\n",
    "                                                          0, 1)\n",
    "\n",
    "        return result\n",
    "\n",
    "# #  MinMaxNormalizer类\n",
    "# # -1 - 1 归一化\n",
    "class meanNormalizer(Normalization):\n",
    "\n",
    "    def transform(self, data):\n",
    "        # 注意传入的是一个data对象\n",
    "        data_input = data.data\n",
    "        data_max = data.data_max\n",
    "        data_min = data.data_min\n",
    "        data_mean = data.data_mean\n",
    "        \n",
    "        data_interval = data_max - data_min\n",
    "        # print(self.data)\n",
    "        print(\"data description >>>\", data_max, data_min)\n",
    "        # TODO: xiaojiao, 需要重写这个算法\n",
    "        \n",
    "        len_df1 = len(data_input)\n",
    "        cols = len(data_input[0]) - 1\n",
    "        results = [None] * len_df1\n",
    "        index  = 0\n",
    "        # for i in range(len_df1):\n",
    "            # col = [row[i] for row in df1_data]\n",
    "        colnames = None\n",
    "        for row in data_input:\n",
    "            if len(row) != len_df1:\n",
    "                pass\n",
    "            # 判断是否空行, None等等\n",
    "            if len(row) == 0:\n",
    "                continue\n",
    "            # 如果第一列有列名，去除列名\n",
    "            # if False:\n",
    "            #     colnames = col.pop(0)\n",
    "            # print(\"col >>\", col)\n",
    "            # col_with_number = list(map(eval, col))\n",
    "            # col_with_number.pop(0)\n",
    "            \n",
    "            # print(col_with_number)  \n",
    "\n",
    "        \n",
    "            # 去除空值\n",
    "            mean = [(x - data_min) / data_interval for x in row if x]\n",
    "            results[index] = mean\n",
    "            index += 1\n",
    "        # return statistics.mean(means)\n",
    "        # return MIN + (MAX - MIN) / (d_max - d_min) * (data - d_min)\n",
    "        results = [x for x in results if x is not None]\n",
    "        return results\n",
    "\n",
    "# ZScoreNormalizer类  \n",
    "class ZScoreNormalizer(Normalization):\n",
    "\n",
    "    def transform(self, data):\n",
    "        data_input = data.data\n",
    "        data_mean = data.data_mean\n",
    "        data_std = data.data_std\n",
    "        \n",
    "        len_df1 = len(data_input)\n",
    "        cols = len(data_input[0]) - 1\n",
    "        means = [None] * len_df1\n",
    "        index  = 0\n",
    "        # for i in range(len_df1):\n",
    "            # col = [row[i] for row in df1_data]\n",
    "        colnames = None\n",
    "        for row in data_input:\n",
    "            # if len(row) != len_df1:\n",
    "            #     pass\n",
    "            # 判断是否空行, None等等\n",
    "            if len(row) == 0:\n",
    "                continue\n",
    "        \n",
    "            # 去除空值\n",
    "            # z = (x - mean) / std\n",
    "            mean = [(x - data_mean) / data_std for x in row if x]\n",
    "            means[index] = mean\n",
    "            index += 1\n",
    "        # print(f'Z-score of {x} is {z}')\n",
    "        means = [x for x in means if x is not None]\n",
    "        return means\n",
    "   \n",
    "# LogNormalizer类data_max\n",
    "class Log1pNormalizer(Normalization):\n",
    "\n",
    "    def transform(self, data):\n",
    "        data_input = data.data\n",
    "        data_mean = data.data_mean\n",
    "        data_std = data.data_std\n",
    "        \n",
    "        len_df1 = len(data_input)\n",
    "        cols = len(data_input[0]) - 1\n",
    "        means = [None] * len_df1\n",
    "        index  = 0\n",
    "        # for i in range(len_df1):\n",
    "            # col = [row[i] for row in df1_data]\n",
    "        colnames = None\n",
    "        for row in data_input:\n",
    "            # if len(row) != len_df1:\n",
    "            #     pass\n",
    "            # 判断是否空行, None等等\n",
    "            if len(row) == 0:\n",
    "                continue\n",
    "        \n",
    "            # 去除空值\n",
    "            # z = (x - mean) / std\n",
    "            mean = [math.log1p(x) for x in row if x]\n",
    "            means[index] = mean\n",
    "            index += 1\n",
    "        # print(f'Z-score of {x} is {z}')\n",
    "        means = [x for x in means if x is not None]\n",
    "        return means\n",
    "\n",
    "class Log2Normalizer(Normalization):\n",
    "\n",
    "    def transform(self, data):\n",
    "        data_input = data.data\n",
    "        data_mean = data.data_mean\n",
    "        data_std = data.data_std\n",
    "        \n",
    "        len_df1 = len(data_input)\n",
    "        cols = len(data_input[0]) - 1\n",
    "        means = [None] * len_df1\n",
    "        index  = 0\n",
    "        # for i in range(len_df1):\n",
    "            # col = [row[i] for row in df1_data]\n",
    "        colnames = None\n",
    "        for row in data_input:\n",
    "            # if len(row) != len_df1:\n",
    "            #     pass\n",
    "            # 判断是否空行, None等等\n",
    "            if len(row) == 0:\n",
    "                continue\n",
    "        \n",
    "            # 去除空值\n",
    "            # z = (x - mean) / std\n",
    "            try:\n",
    "                mean = [math.log2(x) for x in row if x]\n",
    "            except ValueError:\n",
    "                raise ValueError(f'{row}, 有0值')\n",
    "            means[index] = mean\n",
    "            index += 1\n",
    "        # print(f'Z-score of {x} is {z}')\n",
    "        means = [x for x in means if x is not None]\n",
    "        return means\n",
    "\n",
    "\n",
    "class Log10Normalizer(Normalization):\n",
    "\n",
    "    def transform(self, data):\n",
    "        data_input = data.data\n",
    "        data_mean = data.data_mean\n",
    "        data_std = data.data_std\n",
    "        \n",
    "        len_df1 = len(data_input)\n",
    "        cols = len(data_input[0]) - 1\n",
    "        means = [None] * len_df1\n",
    "        index  = 0\n",
    "        # for i in range(len_df1):\n",
    "            # col = [row[i] for row in df1_data]\n",
    "        colnames = None\n",
    "        for row in data_input:\n",
    "            # if len(row) != len_df1:\n",
    "            #     pass\n",
    "            # 判断是否空行, None等等\n",
    "            if len(row) == 0:\n",
    "                continue\n",
    "        \n",
    "            # 去除空值\n",
    "            # z = (x - mean) / std\n",
    "            try:\n",
    "                mean = [math.log10(x) for x in row if x]\n",
    "            except ValueError:\n",
    "                raise ValueError(f'{row}, 有0值')\n",
    "            means[index] = mean\n",
    "            index += 1\n",
    "        # print(f'Z-score of {x} is {z}')\n",
    "        means = [x for x in means if x is not None]\n",
    "        return means\n",
    "    \n",
    "   \n",
    "# # 5. center 中心值归一化\n",
    "class CenterNormalizer(Normalization):\n",
    "    \n",
    "    def transform(self, data):\n",
    "        # mean = np.mean(data)\n",
    "        data_input = data.data\n",
    "        data_mean = data.data_mean\n",
    "        data_std = data.data_std\n",
    "        \n",
    "        len_df1 = len(data_input)\n",
    "        cols = len(data_input[0]) - 1\n",
    "        means = [None] * len_df1\n",
    "        index  = 0\n",
    "        # for i in range(len_df1):\n",
    "            # col = [row[i] for row in df1_data]\n",
    "        colnames = None\n",
    "        for row in data_input:\n",
    "            if len(row) != len_df1:\n",
    "                pass\n",
    "            # 判断是否空行, None等等\n",
    "            if len(row) == 0:\n",
    "                continue\n",
    "        \n",
    "            # 去除空值\n",
    "            # z = (x - mean) / std\n",
    "            mean = [x - data_mean for x in row if x]\n",
    "            means[index] = mean\n",
    "            index += 1\n",
    "        # print(f'Z-score of {x} is {z}')\n",
    "        means = [x for x in means if x is not None]\n",
    "        return means\n",
    "        return (data - mean)\n",
    "\n",
    "# #  6. 帕莱托分布 归一化\n",
    "class ParetoNormalizer(Normalization):\n",
    "\n",
    "    def transform(self, data):\n",
    "        data_input = data.data\n",
    "        data_mean = data.data_mean\n",
    "        data_std = data.data_std\n",
    "        \n",
    "        len_df1 = len(data_input)\n",
    "        cols = len(data_input[0]) - 1\n",
    "        means = [None] * len_df1\n",
    "        index  = 0\n",
    "        # for i in range(len_df1):\n",
    "            # col = [row[i] for row in df1_data]\n",
    "        colnames = None\n",
    "        for row in data_input:\n",
    "            if len(row) != len_df1:\n",
    "                pass\n",
    "            # 判断是否空行, None等等\n",
    "            if len(row) == 0:\n",
    "                continue\n",
    "        \n",
    "            # 去除空值\n",
    "            # z = (x - mean) / std\n",
    "            mean = [(x - data_mean) / data_std for x in row if x]\n",
    "            means[index] = mean\n",
    "            index += 1\n",
    "        # print(f'Z-score of {x} is {z}')\n",
    "        means = [x for x in means if x is not None]\n",
    "        return means\n",
    "        # mean = data.mean()\n",
    "        # std = data.std()\n",
    "\n",
    "        # normalized = (data - mean) / std\n",
    "        # normalized = (data - min_) / (max_ - min_)  \n",
    "        # normalized = 1 - (1/normalized)**scale\n",
    "        # return normalized\n",
    "\n",
    "# 8. 相对丰度按行计算\n",
    "class RelativeAbundanceNormalizer(Normalization):\n",
    "\n",
    "    def transform(self, data):\n",
    "        data_input = data.data\n",
    "        data_mean = data.data_mean\n",
    "        data_std = data.data_std\n",
    "        \n",
    "        len_df1 = len(data_input)\n",
    "        cols = len(data_input[0]) - 1\n",
    "        means = [None] * len_df1\n",
    "        index  = 0\n",
    "        # for i in range(len_df1):\n",
    "            # col = [row[i] for row in df1_data]\n",
    "        colnames = None\n",
    "        for row in data_input:\n",
    "            # if len(row) != len_df1:\n",
    "            #     pass\n",
    "            # 判断是否空行, None等等\n",
    "            if len(row) == 0:\n",
    "                continue\n",
    "        \n",
    "            # 去除空值\n",
    "            # z = (x - mean) / std\n",
    "            try:\n",
    "                row_total = sum(row)\n",
    "                mean = [x / row_total for x in row if x]\n",
    "            except ValueError:\n",
    "                raise ValueError(\"数据格式错误，请参考示例文件检查数据文件是否正确。\")\n",
    "            means[index] = mean\n",
    "            index += 1\n",
    "        # print(f'Z-score of {x} is {z}')\n",
    "        means = [x for x in means if x is not None]\n",
    "        return means \n",
    "\n",
    "# 工厂\n",
    "# 其他归一化类\n",
    "\n",
    "class NormalizationFactory:\n",
    "\n",
    "    normalizer_map = {\n",
    "        'min-max': MinMaxNormalizer,\n",
    "        'Z-score': ZScoreNormalizer,\n",
    "        'log(1+x)': Log1pNormalizer,\n",
    "        'log2(x)': Log2Normalizer,\n",
    "        'log10(x)': Log10Normalizer,\n",
    "        'center': CenterNormalizer,\n",
    "        'Pareto': ParetoNormalizer,\n",
    "        'mean':meanNormalizer,\n",
    "        '相对丰度': RelativeAbundanceNormalizer\n",
    "    }\n",
    "\n",
    "    def get_normalizer(self, name):\n",
    "        normalizer_class = self.normalizer_map[name]\n",
    "        return normalizer_class()\n",
    "\n",
    "# author by : www.runoob.com\n",
    "# https://www.runoob.com/python3/python3-check-is-number.html\n",
    "def is_number(s):\n",
    "    \"\"\"判断字符串由数字组组成\n",
    "\n",
    "    Args:\n",
    "        s (_type_): string\n",
    "\n",
    "    Returns:\n",
    "        _type_: bool\n",
    "        \n",
    "    # 测试字符串和数字\n",
    "    print(is_number('foo'))   \n",
    "    # False\n",
    "    print(is_number('1'))    \n",
    "    # True\n",
    "    print(is_number('1.3'))  \n",
    "    # True\n",
    "    print(is_number('-1.37')) \n",
    "    # True\n",
    "    print(is_number('1e3'))  \n",
    "    # True\n",
    "    \"\"\"\n",
    "    try:\n",
    "        float(s.strip())\n",
    "        return True\n",
    "    except ValueError:\n",
    "        pass\n",
    " \n",
    "    # try:\n",
    "    #     import unicodedata\n",
    "    #     unicodedata.numeric(s)\n",
    "    #     return True\n",
    "    # except (TypeError, ValueError):\n",
    "    #     pass\n",
    " \n",
    "    return False\n",
    "\n",
    "\n",
    "def batch_effect_rectification2(input_file, contained_name_with_colname_rowname, parametric_method):\n",
    "    # 参数判断\n",
    "    if not isinstance(input_file, str):\n",
    "        raise ValueError(\"输入文件不是字符串\")\n",
    "    if not isinstance(contained_name_with_colname_rowname, str):\n",
    "        raise ValueError(\"contained_name_with_colname_rowname 不是字符串\")\n",
    "    if not isinstance(parametric_method, str):\n",
    "        raise ValueError(\"parametric_method 不是字符串\")\n",
    "    \n",
    "    choices = [\"行名\", \"列名\", \"都有\", \"都没有\"]\n",
    "    parametric_method_choices = [\"min-max\", \"Z-score\", \"mean\", \"Pareto\", \"center\", \"log(1+x)\", \"log2(x)\", \"log10(x)\", \"相对丰度\"]\n",
    "\n",
    "    if contained_name_with_colname_rowname not in choices:\n",
    "        raise ValueError(\"contained_name_with_colname_rowname 参数错误\") \n",
    "    if parametric_method not in parametric_method_choices:\n",
    "        raise ValueError(\"parametric_method 参数错误\")\n",
    "    # assert parametric_method in parametric_method_choices, \"parametric_method 参数错误\"\n",
    "    \n",
    "    \n",
    "    # print(f\"contained_name_with_colname_rowname <{contained_name_with_colname_rowname}> >>>\", contained_name_with_colname_rowname in choices)\n",
    "\n",
    "    # 将表格字符串转换为列表形式\n",
    "    data = [line.split('/t') for line in input_file.split('/n') if line]\n",
    "\n",
    "    # reader = csv.reader(input_file, delimiter='/t')\n",
    "\n",
    "    # print(\"reader >>>\", reader)\n",
    "    # TODO:xiaojiao, 去除字符串中的空格\n",
    "    len_df1 = len(data)\n",
    "    length_cols = len(data[0]) - 1\n",
    "    rownames = [None] * len_df1\n",
    "    colnames = [None] * length_cols\n",
    "    # colnames = data.pop(0)\n",
    "    index = 0\n",
    "    \n",
    "    # TODO:xiaojiao, 增加contained_name_with_colname_rowname与文件行列情况不对应的状态\n",
    "    # 1. 都有\n",
    "    if contained_name_with_colname_rowname == \"都有\":\n",
    "        index = 0\n",
    "        # print(\">>> data, data\", data)\n",
    "        colnames = data.pop(0)\n",
    "        for line in data:\n",
    "            print(\"line >>> before\", line)\n",
    "            rownames[index] = line.pop(0)\n",
    "            index += 1\n",
    "            # print(\"line >>> line\", line)\n",
    "        # print(\"data >>>\", data)\n",
    "        # data = [[float(x) for x in line] for line in data]\n",
    "        # interval_mapping_normalization_with_nested_matrix(data)\n",
    "    elif contained_name_with_colname_rowname == \"行名\":\n",
    "\n",
    "        for line in data:\n",
    "            rownames[index] = line.pop(0)\n",
    "            index += 1\n",
    "        # data = [[float(x) for x in line] for line in data]\n",
    "        # interval_mapping_normalization_with_nested_matrix(data)\n",
    "    elif contained_name_with_colname_rowname == \"列名\":\n",
    "        colnames = data.pop(0)\n",
    "        # for line in data:\n",
    "        #     rownames[index] = line.pop(0)\n",
    "        # data = [[float(x) for x in line] for line in data]\n",
    "    \n",
    "    elif contained_name_with_colname_rowname == \"都没有\":\n",
    "\n",
    "        # colnames = data.pop(0)\n",
    "        index = 0\n",
    "        # for line in data:\n",
    "        #     rownames[index] = line.pop(0)\n",
    "        # data = [[float(x) for x in line] for line in data]\n",
    "\n",
    "    # 捕获 ValueError\n",
    "    try:\n",
    "        data = [[float(x) for x in line] for line in data]\n",
    "    except ValueError:\n",
    "        raise ValueError(\"数据格式错误，<em>是否包含行列</em>，请参考示例文件检查输入文件格式是否正确。\")\n",
    "    \n",
    "    # 判断是否是矩阵，不包含空值\n",
    "    for row in data:\n",
    "        # if None in row:\n",
    "        #     raise ValueError(\"数据格式错误，请参考示例文件检查数据文件是否正确。\")\n",
    "        if len(row) != length_cols:\n",
    "            raise ValueError(\"数据格式错误, 包含空值, 请参考示例文件检查数据文件是否正确。\")\n",
    "    # 去除None\n",
    "    rownames = [x for x in rownames if x is not None]\n",
    "    # print(data)\n",
    "    print(\"sucess\")\n",
    "    # 变成一维数组\n",
    "    data2 = [float(x) for line in input_file.split('/n')\n",
    "                 for x in line.split('/t') if line\n",
    "                 if is_number(x)]\n",
    "\n",
    "    data_std = statistics.stdev(data2)\n",
    "    data_mean = statistics.mean(data2)\n",
    "    data_variance = statistics.variance(data2)\n",
    "    data_max = max(data2)\n",
    "    data_min = min(data2)\n",
    "    \n",
    "    data_with_description =  DataWithDescription(data)\n",
    "    data_with_description.data_max = data_max\n",
    "    data_with_description.data_min = data_min\n",
    "    data_with_description.data_mean = data_mean\n",
    "    data_with_description.data_std = data_std\n",
    "    data_with_description.data_variance = data_variance\n",
    "    data_with_description.rownames = rownames\n",
    "    data_with_description.colnames = colnames\n",
    "\n",
    "    print(\">>> rownames\", rownames)\n",
    "\n",
    "    # print(\"len_df1 >>>\", df1_data)\n",
    "    print(\"parametric_method >>>\", parametric_method)\n",
    "    \n",
    "    # 调用\n",
    "    factory = NormalizationFactory()\n",
    "    norm = factory.get_normalizer(parametric_method)\n",
    "    result = norm.transform(data_with_description)\n",
    "\n",
    "    # norm.transform(data)\n",
    "    print(\"norm >>>\", result)\n",
    "    temp = result\n",
    "    # 将结果转换为制表符分隔的字符串\n",
    "    # insert(0,)\n",
    "    # result_str = ''.join(['/t'.join(str(row)) for row in colnames]) + '/n'\n",
    "    \n",
    "    print(\"colnames >>>\", colnames)\n",
    "    colnames = [str(x) if x else \"\" for x in colnames]\n",
    "    if contained_name_with_colname_rowname == \"行名\" or contained_name_with_colname_rowname == \"都有\":\n",
    "        index = 0\n",
    "        for row in temp:\n",
    "            row.insert(0, rownames[index]) \n",
    "            index += 1\n",
    "            print(\">>> row\", row)\n",
    "    if contained_name_with_colname_rowname == \"列名\" or contained_name_with_colname_rowname == \"都有\":\n",
    "        temp.insert(0, colnames)\n",
    "\n",
    "\n",
    "    result_str = '/n'.join(['/t'.join(str(row)) for row in temp]) + '/n'\n",
    "    # string = '/n'.join(['/t'.join([str(x) for x in row]) for row in temp])\n",
    "    string = '/n'.join(['/t'.join([str(x) if x else \"\" for x in row])  \n",
    "                     for row in temp])\n",
    "\n",
    "    return dict(\n",
    "        content = string,\n",
    "        type=\"table\",\n",
    "        name=\"归一化后的表格\"\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. 首先判断data是否是空值\n",
    "    # 读取csv文件 \n",
    "    filepath = f'C:/Users/Cherry/Desktop/geekbang/bioinformatics/learnD3/Iris1.txt'\n",
    "\n",
    "    # 指定第一列作为索引\n",
    "    df = pd.read_table(filepath, sep = \"/t\", index_col = 0, header = 0)\n",
    "    batch_effect_rectification2('C:/Users/Cherry/Desktop/geekbang/bioinformatics/learnD3/Iris1.txt',\n",
    "                # \"C:/Users/Cherry/Desktop/geekbang/bioinformatics/learnD3/\",\n",
    "                contained_name_with_colname_rowname =\"都有\",\n",
    "                parametric_method=\"min-max\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statistics\n",
    "\n",
    "# 创建DataFrame\n",
    "data = {'#sample': ['sample1', 'sample2', 'sample3', 'sample4', 'sample5', 'sample6',\n",
    "                    'sample7', 'sample8', 'sample9', 'sample10', 'sample11', 'sample12'],\n",
    "        'env1': [8.54, 8.56, 8.55, 8.56, 8.66, 8.54, 8.59, 8.59, 8.22, 8.27, 8.34, 8.44],\n",
    "        'env2': [55.84, 71.09, 84.96, 81.9, 77.56, 91.25, 92.59, 81.99, 30.89, 29.47, 30.06, 31.89],\n",
    "        'env3': [90.46, 114.65, 108.79, 131.63, 74.37, 74.86, 63.54, 46.43, 31.44, 30.69, 29.2, 35.0],\n",
    "        'env4': [81.63, 129.05, 135.03, 124.57, 131.66, 132.85, 132.28, 122.32, 39.53, 35.53, 36.15, 38.73],\n",
    "        'env5': [1.87, 2.13, 2.23, 2.16, 2.09, 1.99, 2.07, 2.11, 1.63, 1.57, 1.59, 1.67],\n",
    "        'env6': [2.09, 1.89, 2.35, 2.53, 2.04, 1.83, 1.76, 4.28, 3.75, 3.51, 2.77, 3.28],\n",
    "        'env7': [1.38, 1.14, 1.28, 1.16, 2.56, 3.04, 2.45, 2.46, 2.94, 1.74, 2.54, 3.9],\n",
    "        'env8': [47.42, 74.75, 71.42, 81.25, 36.3, 33.0, 35.5, 33.2, 18.1, 19.9, 16.5, 20.6],\n",
    "        'env9': [489.94, 831.34, 808.74, 842.64, 429.8, 420.2, 404.5, 385.3, 116.0, 112.4, 110.0, 123.2]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 使用Pandas计算Z-score\n",
    "for i in range(1,10):\n",
    "    col_name = f'env{i}'\n",
    "    zscore_col_name = f'{col_name}_zscore_pandas'\n",
    "    df[zscore_col_name] = (df[col_name] - df[col_name].mean()) / df[col_name].std()\n",
    "\n",
    "# 使用标准库计算Z-score\n",
    "for i in range(1,10):\n",
    "    col_name = f'env{i}'\n",
    "    data_col = df[col_name]\n",
    "    mean_col = statistics.mean(data_col)\n",
    "    stdev_col = statistics.stdev(data_col)\n",
    "    zscore_col = [(x - mean_col) / stdev_col for x in data_col]\n",
    "    zscore_col_name = f'{col_name}_zscore_stdlib'\n",
    "    df[zscore_col_name] = zscore_col\n",
    "\n",
    "# 比较两种方法的结果\n",
    "comparison_columns = [f'env{i}' for i in range(1,10)]\n",
    "comparison_columns.extend([f'env{i}_zscore_pandas' for i in range(1,10)])\n",
    "comparison_columns.extend([f'env{i}_zscore_stdlib' for i in range(1,10)])\n",
    "comparison = df[comparison_columns]\n",
    "comparison.to_csv(\"comparison.xls\", sep=\"/t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_table(\"C:/Users/Cherry/Downloads/Species_M4.txt\", header=0, sep=\"/t\") \n",
    "# print(df)\n",
    "# 计算每行的值之和\n",
    "df[\"sum\"] = df.sum(axis=1)\n",
    "\n",
    "# 去除和为0的行\n",
    "df = df[df[\"sum\"] != 0]\n",
    "\n",
    "# 删除临时列\n",
    "df.drop(\"sum\", axis=1, inplace=True)\n",
    "\n",
    "\n",
    "first_column = df.iloc[:, 0]\n",
    "# 获取除去第一列的数据\n",
    "data_without_first_column = df.iloc[:, 1:]\n",
    "# 将非数字值转换为 NaN\n",
    "data_without_first_column = data_without_first_column.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# 将字符串转为float\n",
    "df = data_without_first_column\n",
    "df = df.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# \n",
    "print(10 ** 9)\n",
    "temp = 10 ** 5\n",
    "# df = np.power(df, temp)\n",
    "df = df.multiply(temp)\n",
    "# df.divide(df.min())\n",
    "# df = df.replace(0, 1)\n",
    "df = df.applymap(int)\n",
    "# df = np.log1p(df)\n",
    "df_with_new_column = pd.concat([first_column, df.iloc[:, 0:]], axis=1)\n",
    "\n",
    "# np.pow\n",
    "\n",
    "df_with_new_column.to_csv(\"Species_M4_without_0_rows.txt\", sep=\"/t\", index=False, mode=\"w+\")\n",
    "# 输出结果\n",
    "# print(df_with_new_column)\n",
    "int(0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'Xkr4'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6764/2556032794.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[0msample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m             \u001b[0mcounts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgene\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;31m# 提取基因名称\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'Xkr4'"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "counts = defaultdict(lambda: defaultdict(lambda: defaultdict(int))) \n",
    "\n",
    "# 读取文件\n",
    "# df = pd.read_csv('C:/Users/Cherry/Desktop/geekbang/bioinformatics/learnD3/detailed_gene_count_particle.tsv',\n",
    "#                  sep='\\t',\n",
    "#                  header=0)\n",
    "\n",
    "df = pd.read_csv('C:/Users/Cherry/Desktop/geekbang/bioinformatics/learnD3/detailed_gene_count.tsv',\n",
    "                 sep='\\t',\n",
    "                 header=0)\n",
    "\n",
    "\n",
    "# 读取文件 \n",
    "with open('C:/Users/Cherry/Desktop/geekbang/bioinformatics/learnD3/detailed_gene_count.tsv') as f:\n",
    "    lines = f.readlines()\n",
    "    # lines = f.readlines()\n",
    "    headers = lines[0].split('\\t')\n",
    "    \n",
    "    for line in lines[1:]:\n",
    "        values = line.split('\\t')\n",
    "        gene = values[0] \n",
    "        for i, value in enumerate(values[1:]):\n",
    "            sample = headers[i + 1]\n",
    "            counts[gene][sample] = int(value)\n",
    "  \n",
    "# 提取基因名称 \n",
    "genes = [line.split('\\t')[0] for line in lines[1:]]\n",
    "\n",
    "# # 使用字典计数\n",
    "# counts = {}\n",
    "# for gene in genes:\n",
    "#     counts[gene] = counts.get(gene, 0) + 1\n",
    "\n",
    "\n",
    "print(counts)\n",
    "# 使用Counter统计每个基因名称的计数\n",
    "counts = Counter(genes)\n",
    "\n",
    "# 输出统计结果\n",
    "print(counts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
